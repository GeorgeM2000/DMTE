{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeM2000/DMTE/blob/master/code/DMTE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm_6DHW8WU30"
      },
      "source": [
        "# ***Libraries & Tools***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDqukMh63rB7"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "import zipfile\n",
        "import gc\n",
        "\n",
        "\n",
        "from math import pow\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs_tnKOwef_Q"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt0e5GPf8RBx"
      },
      "source": [
        "# ***Global Variables and General Functionality***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iTSFdmS8THg"
      },
      "outputs": [],
      "source": [
        "MAX_LEN=300\n",
        "MAX_LENS = [] # List to hold the values for multiple execution\n",
        "neg_table_size=1000000\n",
        "NEG_SAMPLE_POWER=0.75\n",
        "batch_size=64\n",
        "num_epoch=50 # Default: 200\n",
        "embed_size=200\n",
        "word_embed_size=200\n",
        "lr=1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y-INwI4KbIs"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"arxiv\"\n",
        "data_text_file = \"RAKE5.txt\"\n",
        "data_text_files = [\"RAKE5.txt\", \"data.txt\", \"YAKE5.txt\"]\n",
        "graph_file = 'graph.txt'\n",
        "parent_path = f'/content/datasets/{dataset_name}'\n",
        "log_file = 'DMTE_Execution_Logs.txt'\n",
        "link_pred_results_file = 'DMTE_Link_Pred_Res.txt'\n",
        "node_clf_results_file = 'DMTE_Node_Clf_Res.txt'\n",
        "categories_file = 'group.txt'\n",
        "\n",
        "\n",
        "split_graph_file = 'sgraph15.txt'\n",
        "split_graph_files = ['sgraph15.txt', 'sgraph45.txt', 'sgraph75.txt']\n",
        "test_graph_file = 'tgraph85.txt'\n",
        "test_graph_files = ['tgraph85.txt', 'tgraph55.txt', 'tgraph25.txt']\n",
        "\n",
        "ratio = 0.15\n",
        "\n",
        "# Original parameters\n",
        "alpha = 0.3\n",
        "beta = 0.1\n",
        "\n",
        "# Additional parameters\n",
        "gamma = 0.0\n",
        "delta = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_ratio = [0.15, 0.45, 0.75]\n",
        "clf_num = 5\n",
        "train_classifier = True"
      ],
      "metadata": {
        "id": "IrEKXhzLsLbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xEhbCPl7UQY"
      },
      "outputs": [],
      "source": [
        "for tf in data_text_files:\n",
        "  max_word_count = 0\n",
        "  min_word_count = float('inf')\n",
        "\n",
        "  with open(f'{parent_path}/{tf}', 'r') as file:\n",
        "      for line in file:\n",
        "          word_count = len(line.split())\n",
        "\n",
        "          if word_count > max_word_count:\n",
        "              max_word_count = word_count\n",
        "\n",
        "          if word_count < min_word_count:\n",
        "              min_word_count = word_count\n",
        "\n",
        "  MAX_LENS.append(max_word_count+1)\n",
        "  print(f'=== {tf} ===')\n",
        "  print(\"Max word count:\", max_word_count)\n",
        "  print(\"Min word count:\", min_word_count)\n",
        "  print()\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENS"
      ],
      "metadata": {
        "id": "BVA8S-AD15v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = MAX_LENS[-1] # For single execution"
      ],
      "metadata": {
        "id": "TVz6nGd2TqOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQf-Hk3EljwP"
      },
      "source": [
        "Execute the code below if the data file with the astracts is too large and needs extracting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdXCdfP_-hHD"
      },
      "outputs": [],
      "source": [
        "# Open and extract the zip file\n",
        "with zipfile.ZipFile('/content/PartialData.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(parent_path)\n",
        "\n",
        "print(\"Extraction complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcWCLJQxVIlg"
      },
      "outputs": [],
      "source": [
        "def sub_Mat(P, node):\n",
        "\n",
        "    sub_P = np.zeros((len(node),len(node)))\n",
        "    for i in range(len(node)):\n",
        "        for j in range(len(node)):\n",
        "            sub_P[i,j] = P[node[i],node[j]]\n",
        "\n",
        "    return sub_P"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_list = []\n",
        "for i in range(0, embed_size):\n",
        "    zero_list.append(0)\n",
        "zero_list = np.array(zero_list)"
      ],
      "metadata": {
        "id": "8LriTxLtsco6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectors_from_file(file_path):\n",
        "  vectors = {}\n",
        "\n",
        "  with open(f'{file_path}', \"r\") as f:\n",
        "      for idx, line in enumerate(f):\n",
        "          vector = list(map(float, line.strip().split()))  # Convert to list of floats\n",
        "          vectors[idx] = vector  # Assign embedding to node idx\n",
        "\n",
        "  return vectors"
      ],
      "metadata": {
        "id": "wln9y_qhsdMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the python code below only for node classification tasks"
      ],
      "metadata": {
        "id": "32e7ZLe62AdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the edge list. Store the unique nodes in the list \"nodes\"\n",
        "with open(f'{parent_path}/{graph_file}', 'r') as f:\n",
        "  eedges = f.readlines()\n",
        "\n",
        "edge_list = []\n",
        "nodes = [] # \"nodes\" will contain all the unique nodes of the graph\n",
        "for ee in eedges:\n",
        "  edge_list.append(list(ee.split()))\n",
        "for ll in edge_list:\n",
        "  for ed in ll:\n",
        "    if ed not in nodes:\n",
        "      nodes.append(ed)\n",
        "    else:\n",
        "      continue"
      ],
      "metadata": {
        "id": "srZIUGeIs18y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(nodes)"
      ],
      "metadata": {
        "id": "gt5CDEHZ2K9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(edge_list)"
      ],
      "metadata": {
        "id": "-I94kvG62NN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwNH2N0iVJ1Y"
      },
      "source": [
        "# ***DataSet***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KcJkcNBs_G3"
      },
      "outputs": [],
      "source": [
        "class dataSet:\n",
        "    def __init__(self, text_path, graph_path, labels_path=None):\n",
        "\n",
        "        text_file, graph_file = self.load(text_path, graph_path)\n",
        "        self.edges = self.load_edges(graph_file)\n",
        "        self.text, self.num_vocab, self.num_nodes = self.load_text(text_file)\n",
        "        self.nodes = range(0, self.num_nodes)\n",
        "        self.negative_table = InitNegTable(self.edges)\n",
        "        self.P = self.P_matrix(self.edges, self.num_nodes)\n",
        "\n",
        "\n",
        "    def load(self, text_path, graph_path):\n",
        "        text_file = open(text_path, 'rb').readlines()\n",
        "        graph_file = open(graph_path, 'rb').readlines()\n",
        "\n",
        "        return text_file, graph_file\n",
        "\n",
        "    def load_edges(self, graph_file):\n",
        "        edges = []\n",
        "        for i in graph_file:\n",
        "            if np.random.uniform(0.0, 1.0) <= ratio:\n",
        "                edges.append(list(map(int, i.strip().decode().split('\\t'))))\n",
        "\n",
        "        return edges\n",
        "\n",
        "\n",
        "\n",
        "    # This method has been modified to be compatible with newer versions\n",
        "    def load_text(self, text_file):\n",
        "        \"\"\"\n",
        "        Adapting with adapt(text_data):\n",
        "\n",
        "        vectorize_layer.adapt(text_data) analyzes text_data, builds a vocabulary, and assigns a unique integer ID to each word based on its frequency (most frequent words get lower IDs).\n",
        "        Transforming with vectorize_layer(text_data):\n",
        "\n",
        "        This maps each word in text_data to its corresponding integer token ID, producing a 2D array where each row represents a sequence of token IDs for a given input line, padded or truncated to max_len.\n",
        "        \"\"\"\n",
        "        vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "            max_tokens=None,  # Set a limit if needed\n",
        "            output_mode='int',\n",
        "            output_sequence_length=MAX_LEN\n",
        "        )\n",
        "\n",
        "        text_data = [line.strip() for line in text_file]\n",
        "\n",
        "        vectorize_layer.adapt(text_data)\n",
        "\n",
        "        text = vectorize_layer(text_data).numpy()\n",
        "\n",
        "        num_vocab = len(vectorize_layer.get_vocabulary())\n",
        "        #print(f'Vocabulary: {num_vocab}')\n",
        "        num_nodes = len(text)\n",
        "\n",
        "        return text, num_vocab, num_nodes\n",
        "\n",
        "    def negative_sample(self, edges):\n",
        "        # edges is the sample_edges in self.generate_batches()\n",
        "        node1, node2 = zip(*edges)\n",
        "        sample_edges = []\n",
        "\n",
        "        # The negative table contains edges that don not exist\n",
        "        func = lambda: self.negative_table[random.randint(0, neg_table_size - 1)] # Pick a random node from the negative table\n",
        "\n",
        "        # For each edge...\n",
        "        for i in range(len(edges)):\n",
        "            neg_node = func() # Pick a negative node\n",
        "\n",
        "            # If the negative node is identical to the first and second node in the current edge...\n",
        "            while node1[i] == neg_node or node2[i] == neg_node:\n",
        "                neg_node = func() # Pick another negative node until the neg node is different than the first and second node in the current edge\n",
        "\n",
        "            # Create a new type of edge that has an additional node, the negative node\n",
        "            sample_edges.append([node1[i], node2[i], neg_node])\n",
        "\n",
        "        return sample_edges\n",
        "\n",
        "    def generate_batches(self, mode=None):\n",
        "\n",
        "        num_batch = len(self.edges) // batch_size\n",
        "        edges = self.edges\n",
        "        if mode == 'add':\n",
        "            num_batch += 1\n",
        "            edges.extend(edges[:(batch_size - len(self.edges) % batch_size)])\n",
        "        if mode != 'add':\n",
        "            random.shuffle(edges)\n",
        "        sample_edges = edges[:num_batch * batch_size]\n",
        "\n",
        "        # For each edge in \"sample_edges\", add a negative edge\n",
        "        sample_edges = self.negative_sample(sample_edges)\n",
        "\n",
        "        # Create batches of edges\n",
        "\n",
        "        \"\"\"\n",
        "        The first batch range is 0 -- batch_size - 1\n",
        "        The second batch range is batch_size -- 2 * batch_size - 1\n",
        "        The third batch range is 2 * batch_size -- 3* batch_size - 1 and so on\n",
        "        \"\"\"\n",
        "        batches = []\n",
        "        for i in range(num_batch):\n",
        "            batches.append(sample_edges[i * batch_size:(i + 1) * batch_size])\n",
        "\n",
        "        return batches\n",
        "\n",
        "    def nodes_batches(self, mode=None):\n",
        "\n",
        "        num_batch = len(self.nodes) // batch_size\n",
        "        nodes = self.nodes\n",
        "        if mode == 'add':\n",
        "            num_batch += 1\n",
        "            nodes.extend(nodes[:(batch_size - len(self.nodes) % batch_size)])\n",
        "            random.shuffle(nodes)\n",
        "        if mode != 'add':\n",
        "            random.shuffle(nodes)\n",
        "        sample_nodes = nodes[:num_batch * batch_size]\n",
        "\n",
        "        batches = []\n",
        "        for i in range(num_batch):\n",
        "            batches.append(sample_nodes[i * batch_size:(i + 1) * batch_size])\n",
        "        return batches\n",
        "\n",
        "    def P_matrix(self, edges, num_nodes):\n",
        "        # Take all the edges\n",
        "        a_list,b_list=zip(*edges)\n",
        "        a_list=list(a_list) # This list contains the first nodes in all edges\n",
        "        b_list=list(b_list) # This list contains the second nodes in all edges\n",
        "\n",
        "        P = np.zeros((num_nodes,num_nodes))\n",
        "\n",
        "        for i in range(len(a_list)):\n",
        "            P[a_list[i],b_list[i]]=1 # The prob of transitioning from \"a_list[i]\" to \"b_list[i]\". Initially it's 1 for unweighted graphs\n",
        "            P[b_list[i],a_list[i]]=1 # The prob of transitioning from \"b_list[i]\" to \"a_list[i]\". Initially it's 1 for unweighted graphs\n",
        "\n",
        "        P = normalize(P, axis=1, norm='l1') # We normalize P\n",
        "\n",
        "        return P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AV9wwI_7NSC"
      },
      "source": [
        "# ***DMTE***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S23Tcvj9tQnb"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, vocab_size, num_nodes, alpha, beta, num_labels=None):\n",
        "        # '''hyperparameter'''\n",
        "        with tf.name_scope('read_inputs') as scope:\n",
        "            self.Text_a = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Ta')\n",
        "            self.Text_b = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tb')\n",
        "            self.Text_neg = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tneg')\n",
        "            self.Node_a = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n1')\n",
        "            self.Node_b = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n2')\n",
        "            self.Node_neg = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n3')\n",
        "            self.P_a = tf.compat.v1.placeholder(tf.float32, [batch_size, batch_size], name='Pa')\n",
        "            self.P_b = tf.compat.v1.placeholder(tf.float32, [batch_size, batch_size], name='Pb')\n",
        "            self.P_neg = tf.compat.v1.placeholder(tf.float32, [batch_size, batch_size], name='Pneg')\n",
        "\n",
        "        with tf.name_scope('initialize_embedding') as scope:\n",
        "            self.text_embed = tf.Variable(tf.random.truncated_normal([vocab_size, word_embed_size], stddev=0.3))\n",
        "            self.node_embed = tf.Variable(tf.random.truncated_normal([num_nodes, embed_size // 2], stddev=0.3))\n",
        "            self.node_embed = tf.clip_by_norm(self.node_embed, clip_norm=1, axes=1)\n",
        "\n",
        "        with tf.name_scope('lookup_embeddings') as scope:\n",
        "            self.TA = tf.nn.embedding_lookup(self.text_embed, self.Text_a)\n",
        "            self.T_A = tf.expand_dims(self.TA, -1)\n",
        "\n",
        "            self.TB = tf.nn.embedding_lookup(self.text_embed, self.Text_b)\n",
        "            self.T_B = tf.expand_dims(self.TB, -1)\n",
        "\n",
        "            self.TNEG = tf.nn.embedding_lookup(self.text_embed, self.Text_neg)\n",
        "            self.T_NEG = tf.expand_dims(self.TNEG, -1)\n",
        "\n",
        "            self.N_A = tf.nn.embedding_lookup(self.node_embed, self.Node_a)\n",
        "            self.N_B = tf.nn.embedding_lookup(self.node_embed, self.Node_b)\n",
        "            self.N_NEG = tf.nn.embedding_lookup(self.node_embed, self.Node_neg)\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.convA, self.convB, self.convNeg = self.conv()\n",
        "        self.loss = self.compute_loss()\n",
        "\n",
        "    def conv(self):\n",
        "\n",
        "        W0 = tf.Variable(tf.random.truncated_normal([word_embed_size, embed_size // 2], stddev=0.3))\n",
        "        W1 = tf.Variable(tf.random.truncated_normal([word_embed_size, embed_size // 2], stddev=0.3))\n",
        "        W2 = tf.Variable(tf.random.truncated_normal([word_embed_size, embed_size // 2], stddev=0.3))\n",
        "\n",
        "        # Additional weight matrices\n",
        "        #W3 = tf.Variable(tf.random.truncated_normal([word_embed_size, embed_size // 2], stddev=0.3))\n",
        "        #W4 = tf.Variable(tf.random.truncated_normal([word_embed_size, embed_size // 2], stddev=0.3))\n",
        "\n",
        "        mA = tf.reduce_mean(self.T_A, axis=1, keepdims=True)\n",
        "        mB = tf.reduce_mean(self.T_B, axis=1, keepdims=True)\n",
        "        mNEG = tf.reduce_mean(self.T_NEG, axis=1, keepdims=True)\n",
        "\n",
        "        convA = tf.tanh(tf.squeeze(mA))\n",
        "        convB = tf.tanh(tf.squeeze(mB))\n",
        "        convNEG = tf.tanh(tf.squeeze(mNEG))\n",
        "\n",
        "        attA = (tf.matmul(convA, W0) +\n",
        "                self.alpha * tf.matmul(tf.matmul(self.P_a, convA), W1) +\n",
        "                self.beta * tf.matmul(tf.matmul(tf.math.square(self.P_a), convA), W2))\n",
        "                #gamma * tf.matmul(tf.matmul(tf.math.pow(self.P_a, 3), convA), W3) +\n",
        "                #delta * tf.matmul(tf.matmul(tf.math.pow(self.P_a, 4), convA), W4))\n",
        "\n",
        "        attB = (tf.matmul(convB, W0) +\n",
        "                self.alpha * tf.matmul(tf.matmul(self.P_b, convB), W1) +\n",
        "                self.beta * tf.matmul(tf.matmul(tf.math.square(self.P_b), convB), W2))\n",
        "                #gamma * tf.matmul(tf.matmul(tf.math.pow(self.P_b, 3), convB), W3) +\n",
        "                #delta * tf.matmul(tf.matmul(tf.math.pow(self.P_b, 4), convB), W4))\n",
        "\n",
        "\n",
        "        attNEG = (tf.matmul(convNEG, W0) +\n",
        "                  self.alpha * tf.matmul(tf.matmul(self.P_a, convNEG), W1) +\n",
        "                  self.beta * tf.matmul(tf.matmul(tf.math.square(self.P_a), convNEG), W2))\n",
        "                  #gamma * tf.matmul(tf.matmul(tf.math.pow(self.P_a, 3), convNEG), W3) +\n",
        "                  #delta * tf.matmul(tf.matmul(tf.math.pow(self.P_a, 4), convNEG), W4))\n",
        "\n",
        "        return attA, attB, attNEG\n",
        "\n",
        "    def compute_loss(self):\n",
        "\n",
        "        # Loss functions for:\n",
        "\n",
        "\n",
        "        # Text-Text\n",
        "        p1 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.convA, self.convB), 1)) + 0.001)\n",
        "\n",
        "        p2 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.convA, self.convNeg), 1)) + 0.001)\n",
        "\n",
        "        p11 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.convB, self.convA), 1)) + 0.001)\n",
        "\n",
        "        p12 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.convB, self.convNeg), 1)) + 0.001)\n",
        "\n",
        "\n",
        "\n",
        "        # Node-Node\n",
        "        p3 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.N_A +\n",
        "                                                                 self.alpha * tf.matmul(self.P_a, self.N_A) +\n",
        "                                                                 self.beta * tf.matmul(tf.math.square(self.P_a), self.N_A), self.N_B), 1)) + 0.001)\n",
        "                                                                 #gamma * tf.matmul(tf.math.pow(self.P_a, 3), self.N_A) +\n",
        "                                                                 #delta * tf.matmul(tf.math.pow(self.P_a, 4), self.N_A), self.N_B), 1)) + 0.001)\n",
        "\n",
        "        p4 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.N_A +\n",
        "                                                                  self.alpha * tf.matmul(self.P_a, self.N_A) +\n",
        "                                                                  self.beta * tf.matmul(tf.math.square(self.P_a), self.N_A), self.N_NEG), 1)) + 0.001)\n",
        "                                                                  #gamma * tf.matmul(tf.math.pow(self.P_a, 3), self.N_A) +\n",
        "                                                                  #delta * tf.matmul(tf.math.pow(self.P_a, 4), self.N_A), self.N_NEG), 1)) + 0.001)\n",
        "\n",
        "        p13 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.N_B +\n",
        "                                                                  self.alpha * tf.matmul(self.P_b, self.N_B) +\n",
        "                                                                  self.beta * tf.matmul(tf.math.square(self.P_b), self.N_B), self.N_A), 1)) + 0.001)\n",
        "                                                                  #gamma * tf.matmul(tf.math.pow(self.P_b, 3), self.N_B) +\n",
        "                                                                  #delta * tf.matmul(tf.math.pow(self.P_b, 4), self.N_B), self.N_A), 1)) + 0.001)\n",
        "\n",
        "        p14 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.N_B +\n",
        "                                                                   self.alpha * tf.matmul(self.P_b, self.N_B) +\n",
        "                                                                   self.beta * tf.matmul(tf.math.square(self.P_b), self.N_B), self.N_NEG), 1)) + 0.001)\n",
        "                                                                   #gamma * tf.matmul(tf.math.pow(self.P_b, 3), self.N_B) +\n",
        "                                                                   #delta * tf.matmul(tf.math.pow(self.P_b, 4), self.N_B), self.N_NEG), 1)) + 0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Node-Text\n",
        "        p5 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.N_A +\n",
        "                                                                 self.alpha * tf.matmul(self.P_a, self.N_A) +\n",
        "                                                                 self.beta * tf.matmul(tf.math.square(self.P_a), self.N_A), self.convB), 1)) + 0.001)\n",
        "                                                                 #gamma * tf.matmul(tf.math.pow(self.P_a, 3), self.N_A) +\n",
        "                                                                 #delta * tf.matmul(tf.math.pow(self.P_a, 4), self.N_A), self.convB), 1)) + 0.001)\n",
        "\n",
        "        p6 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.N_A +\n",
        "                                                                  self.alpha * tf.matmul(self.P_a, self.N_A) +\n",
        "                                                                  self.beta * tf.matmul(tf.math.square(self.P_a), self.N_A), self.convNeg), 1)) + 0.001)\n",
        "                                                                  #gamma * tf.matmul(tf.math.pow(self.P_a, 3), self.N_A) +\n",
        "                                                                  #delta * tf.matmul(tf.math.pow(self.P_a, 4), self.N_A), self.convNeg), 1)) + 0.001)\n",
        "\n",
        "        p15 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.N_B +\n",
        "                                                                  self.alpha * tf.matmul(self.P_b, self.N_B) +\n",
        "                                                                  self.beta * tf.matmul(tf.math.square(self.P_b), self.N_B), self.convA), 1)) + 0.001)\n",
        "                                                                  #gamma * tf.matmul(tf.math.pow(self.P_b, 3), self.N_B) +\n",
        "                                                                  #delta * tf.matmul(tf.math.pow(self.P_b, 4), self.N_B), self.convA), 1)) + 0.001)\n",
        "\n",
        "        p16 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.N_B +\n",
        "                                                                   self.alpha * tf.matmul(self.P_b, self.N_B) +\n",
        "                                                                   self.beta * tf.matmul(tf.math.square(self.P_b), self.N_B), self.convNeg), 1)) + 0.001)\n",
        "                                                                   #gamma * tf.matmul(tf.math.pow(self.P_b, 3), self.N_B) +\n",
        "                                                                   #delta * tf.matmul(tf.math.pow(self.P_b, 4), self.N_B), self.convNeg), 1)) + 0.001)\n",
        "\n",
        "\n",
        "\n",
        "        # Text-Node\n",
        "        p7 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.convA, self.N_B), 1)) + 0.001)\n",
        "\n",
        "        p8 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.convA, self.N_NEG), 1)) + 0.001)\n",
        "\n",
        "        p17 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.convB, self.N_A), 1)) + 0.001)\n",
        "\n",
        "        p18 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.convB, self.N_NEG), 1)) + 0.001)\n",
        "\n",
        "        rho1 = 0.7\n",
        "        rho2 = 1.0\n",
        "        rho3 = 0.1\n",
        "        temp_loss = rho1 * (p1 + p2 + p11 + p12) + rho2 * (p3 + p4 + p13 + p14) + rho3 * (p5 + p6 + p15 + p16) + rho3 * (p7 + p8 + p17 + p18)\n",
        "        loss = -tf.reduce_sum(temp_loss)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vt_wvryq0oh"
      },
      "source": [
        "# ***Negative Sample***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdwgdktTq504"
      },
      "outputs": [],
      "source": [
        "def InitNegTable(edges):\n",
        "    a_list, b_list = zip(*edges)\n",
        "    a_list = list(a_list)\n",
        "    b_list = list(b_list)\n",
        "    node = a_list\n",
        "    node.extend(b_list)\n",
        "\n",
        "    node_degree = {}\n",
        "    for i in node:\n",
        "        if i in node_degree:\n",
        "            node_degree[i] += 1\n",
        "        else:\n",
        "            node_degree[i] = 1\n",
        "    sum_degree = 0\n",
        "    for i in node_degree.values():\n",
        "        sum_degree += pow(i, 0.75)\n",
        "\n",
        "    por = 0\n",
        "    cur_sum = 0\n",
        "    vid = -1\n",
        "    neg_table = []\n",
        "    degree_list = list(node_degree.values())\n",
        "    node_id = list(node_degree.keys())\n",
        "    for i in range(neg_table_size):\n",
        "        if ((i + 1) / float(neg_table_size)) > por:\n",
        "            cur_sum += pow(degree_list[vid + 1], NEG_SAMPLE_POWER)\n",
        "            por = cur_sum / sum_degree\n",
        "            vid += 1\n",
        "        neg_table.append(node_id[vid])\n",
        "    print(f'Neg. table size: {len(neg_table)}')\n",
        "    return neg_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Classify***"
      ],
      "metadata": {
        "id": "wYMJC2fys--x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKRanker(OneVsRestClassifier):\n",
        "    def predict(self, X, top_k_list):\n",
        "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
        "        all_labels = []\n",
        "        for i, k in enumerate(top_k_list):\n",
        "            probs_ = probs[i, :]\n",
        "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
        "            probs_[:] = 0\n",
        "            probs_[labels] = 1\n",
        "            all_labels.append(probs_)\n",
        "        return np.asarray(all_labels)\n",
        "\n",
        "\n",
        "class Classifier(object):\n",
        "\n",
        "    def __init__(self, vectors, clf):\n",
        "        self.embeddings = vectors\n",
        "        self.clf = TopKRanker(clf)\n",
        "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
        "\n",
        "    def train(self, X, Y, Y_all):\n",
        "        self.binarizer.fit(Y_all)\n",
        "        # X_train = [self.embeddings[x] for x in X]\n",
        "        X_train = [self.embeddings[int(x)] for x in X] # For each node in X, take its embedding\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        self.clf.fit(X_train, Y)\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        top_k_list = [len(l) for l in Y] # For each label in Y, take its size (multi-label)\n",
        "        Y_ = self.predict(X, top_k_list)\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        averages = [\"micro\", \"macro\"]\n",
        "        results = {}\n",
        "        for average in averages:\n",
        "            results[average] = f1_score(Y, Y_, average=average)\n",
        "        return results\n",
        "\n",
        "    def predict(self, X, top_k_list):\n",
        "        X_ = np.asarray([self.embeddings[int(x)] for x in X])\n",
        "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
        "        return Y\n",
        "\n",
        "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
        "        state = np.random.get_state()\n",
        "        training_size = int(train_precent * len(X)) # Set the ratio based on the size of X\n",
        "        np.random.seed(seed)\n",
        "        shuffle_indices = np.random.permutation(np.arange(len(X))) # Shuffle the indices of X (X contains all nodes)\n",
        "\n",
        "        # Access the values of X and Y based on the shuffled indices\n",
        "\n",
        "        # X_train and Y_train will have \"training_size\" number of values of X and Y\n",
        "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
        "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
        "\n",
        "        # X_test and Y_test will have \"len(X) - training_size\" number of values of X and Y\n",
        "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "\n",
        "        self.train(X_train, Y_train, Y) # Y has the labels of all nodes\n",
        "        np.random.set_state(state)\n",
        "        return self.evaluate(X_test, Y_test)\n",
        "\n",
        "\n",
        "\n",
        "def load_embeddings(filename):\n",
        "    fin = open(filename, 'r')\n",
        "    node_num, size = [int(x) for x in fin.readline().strip().split()]\n",
        "    vectors = {}\n",
        "    while 1:\n",
        "        l = fin.readline()\n",
        "        if l == '':\n",
        "            break\n",
        "        vec = l.strip().split(' ')\n",
        "        assert len(vec) == size + 1\n",
        "        vectors[vec[0]] = [float(x) for x in vec[1:]]\n",
        "    fin.close()\n",
        "    assert len(vectors) == node_num\n",
        "    return vectors\n",
        "\n",
        "def read_node_label(filename):\n",
        "    fin = open(filename, 'r')\n",
        "    X = []\n",
        "    Y = []\n",
        "    XY_dic = {}\n",
        "    X_Y_dic = {}\n",
        "    while 1:\n",
        "        l = fin.readline()\n",
        "        if l == '':\n",
        "            break\n",
        "        # vec = l.strip().split('\\t')\n",
        "        vec = l.strip().split(' ')\n",
        "        X.append(vec[0])\n",
        "        Y.append(vec[1:])\n",
        "        X_Y_dic[str(vec[0])] = str(vec[1:][0])\n",
        "        XY_dic.setdefault(str(vec[1:][0]), []).append(str(vec[0]))\n",
        "    fin.close()\n",
        "    return X, Y, XY_dic, X_Y_dic"
      ],
      "metadata": {
        "id": "877NeCBrtBuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLaqva64WoA0"
      },
      "source": [
        "# ***Run (Single execution)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgbe18H4WsgX"
      },
      "outputs": [],
      "source": [
        "def prepareData(graph, ratio):\n",
        "  with open(f'{parent_path}/{graph}', 'rb') as f:\n",
        "    edges = [i for i in f]\n",
        "\n",
        "  selected = int(len(edges) * float(ratio))\n",
        "  selected = selected - selected % batch_size\n",
        "  selected = random.sample(edges, selected)\n",
        "  remain = [i for i in edges if i not in selected]\n",
        "  try:\n",
        "    temp_dir = Path('temp')\n",
        "\n",
        "    # Check if the directory exists, if so, delete it\n",
        "    if temp_dir.exists() and temp_dir.is_dir():\n",
        "        shutil.rmtree(temp_dir)\n",
        "        print(\"Existing directory deleted.\")\n",
        "\n",
        "    # Create the directory\n",
        "    temp_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(\"Directory created successfully.\")\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "  with open('temp/graph.txt', 'wb') as f:\n",
        "    for i in selected:\n",
        "      f.write(i)\n",
        "\n",
        "  with open('temp/test_graph.txt', 'wb') as f:\n",
        "    for i in remain:\n",
        "      f.write(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK_Z-E1HXRVm"
      },
      "outputs": [],
      "source": [
        "prepareData(graph_file, ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wFgChvWggiu"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "#graph_path = os.path.join('/content/temp/graph.txt') # Use this if you executed the prepareData() function\n",
        "\n",
        "data = dataSet(f'{parent_path}/{data_text_file}',\n",
        "               f'{parent_path}/{split_graph_file}')\n",
        "\n",
        "# Saving embeddings\n",
        "embed_file = f\"{parent_path}/Results/DMTE/embed_link_pred_{split_graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\"\n",
        "#embed_file = f\"{parent_path}/Results/DMTE/embed_node_clf_{graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\" # For node classification the whole graph ('graph.txt') is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QsRmNqLXrGb"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default():\n",
        "    sess = tf.compat.v1.Session()\n",
        "    with sess.as_default():\n",
        "        model = Model(data.num_vocab, data.num_nodes, alpha, beta)\n",
        "        opt = tf.compat.v1.train.AdamOptimizer(lr)#tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        train_op = opt.minimize(model.loss)#opt.minimize(model.loss, var_list=model.trainable_variables)\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        #total_time = 0\n",
        "\n",
        "        # training\n",
        "        print('start training.......')\n",
        "        start_time = datetime.now()\n",
        "        for epoch in tqdm(range(num_epoch), desc=\"Epochs\"):\n",
        "            #start_time = datetime.now()\n",
        "            loss_epoch = 0\n",
        "            batches = data.generate_batches()\n",
        "            h1 = 0\n",
        "            num_batch = len(batches)\n",
        "            for i in range(num_batch):\n",
        "                batch = batches[i]\n",
        "\n",
        "                node1, node2, node3 = zip(*batch)\n",
        "                node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "                P1, P2, P3 = sub_Mat(data.P, node1), sub_Mat(data.P, node2), sub_Mat(data.P, node3)\n",
        "\n",
        "\n",
        "                feed_dict = {\n",
        "                    model.Text_a: text1,\n",
        "                    model.Text_b: text2,\n",
        "                    model.Text_neg: text3,\n",
        "                    model.Node_a: node1,\n",
        "                    model.Node_b: node2,\n",
        "                    model.Node_neg: node3,\n",
        "                    model.P_a: P1,\n",
        "                    model.P_b: P2,\n",
        "                    model.P_neg: P3\n",
        "                }\n",
        "\n",
        "                # run the graph\n",
        "                _, loss_batch = sess.run([train_op, model.loss], feed_dict=feed_dict)\n",
        "                loss_epoch += loss_batch\n",
        "\n",
        "            #end_time = datetime.now()\n",
        "            #total_time += (end_time - start_time).total_seconds()\n",
        "            #print('epoch: ', epoch + 1, ' loss: ', loss_epoch)\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        print(f'Total time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "\n",
        "\n",
        "        # Saving embeddings\n",
        "        with open(embed_file, 'wb') as f:\n",
        "            batches = data.generate_batches(mode='add')\n",
        "            num_batch = len(batches)\n",
        "            embed = [[] for _ in range(data.num_nodes)]\n",
        "\n",
        "            for i in range(num_batch):\n",
        "                batch = batches[i]\n",
        "                node1, node2, node3 = zip(*batch)\n",
        "                node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "                P1, P2, P3 = sub_Mat(data.P, node1), sub_Mat(data.P, node2), sub_Mat(data.P, node3)\n",
        "\n",
        "                feed_dict = {\n",
        "                    model.Text_a: text1,\n",
        "                    model.Text_b: text2,\n",
        "                    model.Text_neg: text3,\n",
        "                    model.Node_a: node1,\n",
        "                    model.Node_b: node2,\n",
        "                    model.Node_neg: node3,\n",
        "                    model.P_a: P1,\n",
        "                    model.P_b: P2,\n",
        "                    model.P_neg: P3\n",
        "                }\n",
        "\n",
        "                # Fetch embeddings\n",
        "                convA, convB, TA, TB = sess.run([model.convA, model.convB, model.N_A, model.N_B], feed_dict=feed_dict)\n",
        "\n",
        "                # For each node in the batch\n",
        "                for j in range(batch_size):\n",
        "                    em = list(convA[j]) + list(TA[j]) # Create an embedding by concatenating the text (convA) and node (TA) embeddings\n",
        "                    embed[node1[j]].append(em) # A node can appear many times in edges. Thus, each time that node will have a different embedding. Append the different embeddings for a particular node\n",
        "\n",
        "                    em = list(convB[j]) + list(TB[j]) # Create an embedding by concatenating the text (convA) and node (TA) embeddings\n",
        "                    embed[node2[j]].append(em)\n",
        "\n",
        "\n",
        "            for i in range(data.num_nodes):\n",
        "                if embed[i]:\n",
        "                    tmp = np.sum(embed[i], axis=0) / len(embed[i]) #np.mean(embed[i], axis=0)  # If a node has many different embeddings, take their mean.\n",
        "                    f.write((' '.join(map(str, tmp)) + '\\n').encode())\n",
        "                else:\n",
        "                    f.write('\\n'.encode())\n",
        "                    #f.write((' '.join(map(str, zero_list)) + '\\n').encode()) # For node classification\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Link Prediction***"
      ],
      "metadata": {
        "id": "D0V55CBqvfOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZsQv5KyY83v",
        "outputId": "da4038c3-fcfb-4580-b33d-5fd406d184b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auc value: 0.8064516129032258\n"
          ]
        }
      ],
      "source": [
        "node2vec = {}\n",
        "with open(embed_file, 'rb') as f:\n",
        "  for i, j in enumerate(f):\n",
        "    if j.decode() != '\\n':\n",
        "      node2vec[i] = list(map(float, j.strip().decode().split(' ')))\n",
        "\n",
        "\n",
        "with open(os.path.join(f'{parent_path}/{test_graph_file}'), 'rb') as f:\n",
        "  edges = [list(map(int, i.strip().decode().split('\\t'))) for i in f]\n",
        "\n",
        "\n",
        "nodes = list(set([i for j in edges for i in j]))\n",
        "a = 0\n",
        "b = 0\n",
        "for i, j in edges:\n",
        "  if i in node2vec.keys() and j in node2vec.keys():\n",
        "    dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "    random_node = random.sample(nodes, 1)[0]\n",
        "    while random_node == j or random_node not in node2vec.keys():\n",
        "        random_node = random.sample(nodes, 1)[0]\n",
        "    dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "    if dot1 > dot2:\n",
        "        a += 1\n",
        "    elif dot1 == dot2:\n",
        "        a += 0.5\n",
        "    b += 1\n",
        "\n",
        "print(\"Auc value:\", float(a) / b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Node Classification***"
      ],
      "metadata": {
        "id": "6QGHD2G3vvs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  tags = f.readlines() # \"tags\" will be a 2D list. Each sublist will have the form: nodeID     label\n",
        "\n",
        "if train_classifier:\n",
        "\n",
        "  clf_test_len = len(nodes) # The number of nodes will be the same in each run since we're using the whole graph and thus, all of its nodes\n",
        "  print('Train classifier start!')\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "  new_vector = get_vectors_from_file(embed_file)\n",
        "\n",
        "  for jk in range(0, clf_test_len):\n",
        "    if str(jk) in nodes: # If the index \"jk\" is a node\n",
        "      tag_list = tags[jk].strip().split() # For node \"jk\", take this info: jk     label\n",
        "      # Y.append([(int)(i) for i in tags])\n",
        "      lli = [str(i) for i in tag_list] # For node \"jk\", lli will contain all of its labels\n",
        "      if len(lli) != 0:\n",
        "        if np.array(new_vector[jk]).any() != np.array(zero_list).any(): # If there is no zero value in the embedding of \"jk\"\n",
        "          X.append(jk)\n",
        "          Y.append(lli[1:][0]) # Take the first label (if there are multiple) of node \"jk\"\n",
        "\n",
        "  # This part of the code uses only the X and Y lists created above\n",
        "  mi = {}\n",
        "  ma = {}\n",
        "  li1 = []\n",
        "  li2 = []\n",
        "  with open(f'{parent_path}/Results/DMTE/{node_clf_results_file}', 'a') as f:\n",
        "    f.write(f'{embed_file.split('/')[-1]} \\n')\n",
        "    print(embed_file.split('/')[-1])\n",
        "    for i in range(0, len(clf_ratio)): # Experiment with each ratio\n",
        "      for j in range(0, clf_num): # clf_num = 5\n",
        "\n",
        "        clf = Classifier(vectors=new_vector, # All node embeddings\n",
        "                        clf=LogisticRegression())\n",
        "\n",
        "        result = clf.split_train_evaluate(X, Y, clf_ratio[i])\n",
        "\n",
        "        # Results\n",
        "        li1.append(result['micro'])\n",
        "        li2.append(result['macro'])\n",
        "\n",
        "\n",
        "      mi[str(str(clf_ratio[i]) + '-micro')] = sum(li1) / clf_num\n",
        "      ma[str(str(clf_ratio[i]) + '-macro')] = sum(li2) / clf_num\n",
        "\n",
        "      print(mi)\n",
        "      print(ma)\n",
        "      print()\n",
        "\n",
        "      f.writelines(str(str(mi)+str(ma)))\n",
        "      f.write('\\n')\n",
        "\n",
        "      # Reinitialize the dictionaries and lists\n",
        "      mi = {}\n",
        "      ma = {}\n",
        "      li1 = []\n",
        "      li2 = []"
      ],
      "metadata": {
        "id": "NHOPbtjivzVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Run (Multiple executions)***"
      ],
      "metadata": {
        "id": "l7BX9wrS3-vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for gf in split_graph_files: # For link prediction. For node classification just use: for gf in ['graph.txt']:\n",
        "    for t, txtf in enumerate(data_text_files):\n",
        "\n",
        "      MAX_LEN = MAX_LENS[t]\n",
        "      print(f'The maximum length is: {MAX_LEN}')\n",
        "\n",
        "      data = dataSet(f'{parent_path}/{txtf}', f'{parent_path}/{gf}')\n",
        "\n",
        "      # Logging the execution details\n",
        "      with open(f'{parent_path}/Results/DMTE/{log_file}', 'a') as f:\n",
        "          f.write(f'Processing graph: {gf}, text: {txtf}\\n')\n",
        "\n",
        "      print(f'Processing graph: {gf}, text: {txtf}')\n",
        "\n",
        "      with tf.Graph().as_default():\n",
        "          sess = tf.compat.v1.Session()\n",
        "          with sess.as_default():\n",
        "              model = Model(data.num_vocab, data.num_nodes, alpha, beta)\n",
        "              opt = tf.compat.v1.train.AdamOptimizer(lr)\n",
        "              train_op = opt.minimize(model.loss)\n",
        "              sess.run(tf.compat.v1.global_variables_initializer())\n",
        "              #total_time = 0\n",
        "\n",
        "              # Training\n",
        "              print('start training.......')\n",
        "              start_time = datetime.now()\n",
        "              for epoch in tqdm(range(num_epoch)):\n",
        "                  #start_time = datetime.now()\n",
        "                  loss_epoch = 0\n",
        "                  batches = data.generate_batches()\n",
        "                  h1 = 0\n",
        "                  num_batch = len(batches)\n",
        "                  for i in range(num_batch):\n",
        "                      batch = batches[i]\n",
        "\n",
        "                      node1, node2, node3 = zip(*batch)\n",
        "                      node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                      text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "                      P1, P2, P3 = sub_Mat(data.P, node1), sub_Mat(data.P, node2), sub_Mat(data.P, node3)\n",
        "\n",
        "\n",
        "                      feed_dict = {\n",
        "                          model.Text_a: text1,\n",
        "                          model.Text_b: text2,\n",
        "                          model.Text_neg: text3,\n",
        "                          model.Node_a: node1,\n",
        "                          model.Node_b: node2,\n",
        "                          model.Node_neg: node3,\n",
        "                          model.P_a: P1,\n",
        "                          model.P_b: P2,\n",
        "                          model.P_neg: P3\n",
        "                      }\n",
        "\n",
        "                      # run the graph\n",
        "                      _, loss_batch = sess.run([train_op, model.loss], feed_dict=feed_dict)\n",
        "                      loss_epoch += loss_batch\n",
        "\n",
        "                  #end_time = datetime.now()\n",
        "                  #total_time += (end - start).total_seconds()\n",
        "                  #print('epoch: ', epoch + 1, ' loss: ', loss_epoch)\n",
        "\n",
        "              end_time = datetime.now()\n",
        "              with open(f'{parent_path}/Results/DMTE/{log_file}', 'a') as f:\n",
        "                  f.write(f'Time: {((end_time - start_time).total_seconds()) / 60.0}\\n')\n",
        "\n",
        "              print(f'Total Time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "\n",
        "              # Save embeddings with a unique name\n",
        "              embed_file = f\"{parent_path}/Results/DMTE/embed_link_pred_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "              #embed_file = f\"{parent_path}/Results/DMTE/embed_node_clf_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "\n",
        "              with open(embed_file, 'wb') as f:\n",
        "                  batches = data.generate_batches(mode='add')\n",
        "                  num_batch = len(batches)\n",
        "                  embed = [[] for _ in range(data.num_nodes)]\n",
        "\n",
        "                  for i in range(num_batch):\n",
        "                      batch = batches[i]\n",
        "                      node1, node2, node3 = zip(*batch)\n",
        "                      node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                      text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "                      P1, P2, P3 = sub_Mat(data.P, node1), sub_Mat(data.P, node2), sub_Mat(data.P, node3)\n",
        "\n",
        "                      feed_dict = {\n",
        "                          model.Text_a: text1,\n",
        "                          model.Text_b: text2,\n",
        "                          model.Text_neg: text3,\n",
        "                          model.Node_a: node1,\n",
        "                          model.Node_b: node2,\n",
        "                          model.Node_neg: node3,\n",
        "                          model.P_a: P1,\n",
        "                          model.P_b: P2,\n",
        "                          model.P_neg: P3\n",
        "                      }\n",
        "\n",
        "                      # Fetch embeddings\n",
        "                      convA, convB, TA, TB = sess.run([model.convA, model.convB, model.N_A, model.N_B], feed_dict=feed_dict)\n",
        "\n",
        "                      # For each node in the batch\n",
        "                      for j in range(batch_size):\n",
        "                          em = list(convA[j]) + list(TA[j]) # Create an embedding by concatenating the text (convA) and node (TA) embeddings\n",
        "                          embed[node1[j]].append(em) # A node can appear many times in edges. Thus, each time that node will have a different embedding. Append the different embeddings for a particular node\n",
        "\n",
        "                          em = list(convB[j]) + list(TB[j]) # Create an embedding by concatenating the text (convA) and node (TA) embeddings\n",
        "                          embed[node2[j]].append(em)\n",
        "\n",
        "\n",
        "                  for i in range(data.num_nodes):\n",
        "                      if embed[i]:\n",
        "                          tmp = np.sum(embed[i], axis=0) / len(embed[i]) # np.mean(embed[i], axis=0) # If a node has many different embeddings, take their mean.\n",
        "                          f.write((' '.join(map(str, tmp)) + '\\n').encode())\n",
        "                      else:\n",
        "                          f.write('\\n'.encode())\n",
        "                          #f.write((' '.join(map(str, zero_list)) + '\\n').encode()) # For node classification\n",
        "\n",
        "              # Log completion\n",
        "              with open(f'{parent_path}/Results/DMTE/{log_file}', 'a') as f:\n",
        "                  f.write(f'Embeddings saved to: {embed_file}\\n')\n",
        "\n",
        "      gc.collect()"
      ],
      "metadata": {
        "id": "qM5K667l4By7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Node Classification***"
      ],
      "metadata": {
        "id": "MNlYivelwyMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_files = [f'{parent_path}/Results/DMTE/embed_node_clf_graph_data.txt']\n",
        "\n",
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  tags = f.readlines() # \"tags\" will be a 2D list. Each sublist will have the form: nodeID     label\n",
        "\n",
        "if train_classifier:\n",
        "\n",
        "  clf_test_len = len(nodes) # The number of nodes will be the same in each run since we're using the whole graph and thus, all of its nodes\n",
        "  print('Train classifier start!')\n",
        "\n",
        "  for ef in embed_files:\n",
        "    X = []\n",
        "    Y = []\n",
        "    new_vector = get_vectors_from_file(ef)\n",
        "\n",
        "    for jk in range(0, clf_test_len):\n",
        "      if str(jk) in nodes: # If the index \"jk\" is a node\n",
        "        tag_list = tags[jk].strip().split() # For node \"jk\", take this info: jk     label\n",
        "        # Y.append([(int)(i) for i in tags])\n",
        "        lli = [str(i) for i in tag_list] # For node \"jk\", lli will contain all of its labels\n",
        "        if len(lli) != 0:\n",
        "          if np.array(new_vector[jk]).any() != np.array(zero_list).any(): # If there is no zero value in the embedding of \"jk\"\n",
        "            X.append(jk)\n",
        "            Y.append(lli[1:][0]) # Take the first label (if there are multiple) of node \"jk\"\n",
        "\n",
        "    # This part of the code uses only the X and Y lists created above\n",
        "    mi = {}\n",
        "    ma = {}\n",
        "    li1 = []\n",
        "    li2 = []\n",
        "    with open(f'{parent_path}/Results/DMTE/{node_clf_results_file}', 'a') as f:\n",
        "      f.write(f'{ef.split('/')[-1]} \\n')\n",
        "      print(ef.split('/')[-1])\n",
        "      for i in range(0, len(clf_ratio)): # Experiment with each ratio\n",
        "        for j in range(0, clf_num): # clf_num = 5\n",
        "\n",
        "          clf = Classifier(vectors=new_vector, # All node embeddings\n",
        "                          clf=LogisticRegression())\n",
        "\n",
        "          result = clf.split_train_evaluate(X, Y, clf_ratio[i])\n",
        "\n",
        "          # Results\n",
        "          li1.append(result['micro'])\n",
        "          li2.append(result['macro'])\n",
        "\n",
        "\n",
        "        mi[str(str(clf_ratio[i]) + '-micro')] = sum(li1) / clf_num\n",
        "        ma[str(str(clf_ratio[i]) + '-macro')] = sum(li2) / clf_num\n",
        "\n",
        "\n",
        "        print(mi)\n",
        "        print(ma)\n",
        "        print()\n",
        "\n",
        "\n",
        "        f.writelines(str(str(mi)+str(ma)))\n",
        "        f.write('\\n')\n",
        "\n",
        "        # Reinitialize the dictionaries and lists\n",
        "        mi = {}\n",
        "        ma = {}\n",
        "        li1 = []\n",
        "        li2 = []\n",
        "\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "psfFEyT7w1Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Link Prediction***"
      ],
      "metadata": {
        "id": "VLY6msD3w1iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_files = [[f'{parent_path}/Results/DMTE/embed_link_pred_sgraph15_RAKE5.txt']]\n",
        "\n",
        "# Initialize a log file to store the AUC results\n",
        "with open(f'{parent_path}/Results/DMTE/{link_pred_results_file}', \"a\") as f:\n",
        "    f.write(\"Embed File\\tAUC Value\\n\")\n",
        "\n",
        "for tgfi, tgf in enumerate(test_graph_files):\n",
        "  for ef in embed_files[tgfi]:\n",
        "      node2vec = {}\n",
        "\n",
        "      # Load the embeddings from the current embed file\n",
        "      with open(ef, 'rb') as f:\n",
        "          for i, j in enumerate(f):\n",
        "              if j.decode().strip():\n",
        "                  node2vec[i] = list(map(float, j.strip().decode().split(' ')))\n",
        "\n",
        "      # Load the edges from the test graph file\n",
        "      with open(f'{parent_path}/{tgf}', 'rb') as f:\n",
        "          edges = [list(map(int, i.strip().decode().split())) for i in f]\n",
        "\n",
        "      nodes = list(set([i for j in edges for i in j]))\n",
        "\n",
        "      # Calculate AUC\n",
        "      a = 0\n",
        "      b = 0\n",
        "      for i, j in edges:\n",
        "          if i in node2vec.keys() and j in node2vec.keys():\n",
        "              dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "              random_node = random.sample(nodes, 1)[0]\n",
        "              while random_node == j or random_node not in node2vec.keys():\n",
        "                  random_node = random.sample(nodes, 1)[0]\n",
        "              dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "              if dot1 > dot2:\n",
        "                  a += 1\n",
        "              elif dot1 == dot2:\n",
        "                  a += 0.5\n",
        "              b += 1\n",
        "\n",
        "      auc_value = float(a) / b if b > 0 else 0\n",
        "      print(f\"AUC value for {ef.split('/')[-1]}: {auc_value}\")\n",
        "\n",
        "      # Log the result\n",
        "      with open(f'{parent_path}/Results/DMTE/{link_pred_results_file}', \"a\") as f:\n",
        "          f.write(f\"{ef}\\t{tgf}\\t{auc_value}\\n\")\n",
        "\n",
        "      gc.collect()"
      ],
      "metadata": {
        "id": "OT0ngnsv4JVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}