{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeM2000/DMTE/blob/master/code/DMTE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm_6DHW8WU30"
      },
      "source": [
        "# ***Libraries & Tools***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDqukMh63rB7"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from math import pow\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs_tnKOwef_Q"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt0e5GPf8RBx"
      },
      "source": [
        "# ***Global Variables and General Functionality***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iTSFdmS8THg"
      },
      "outputs": [],
      "source": [
        "MAX_LEN=300\n",
        "neg_table_size=1000000\n",
        "NEG_SAMPLE_POWER=0.75\n",
        "batch_size=64\n",
        "num_epoch=200 # Default: 200\n",
        "embed_size=200\n",
        "word_embed_size=200\n",
        "lr=1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y-INwI4KbIs"
      },
      "outputs": [],
      "source": [
        "datasetName = \"cora\"\n",
        "dataTextFile = \"data.txt\"\n",
        "ratio = 0.15\n",
        "\n",
        "# Original parameters\n",
        "alpha = 0.3\n",
        "beta = 0.1\n",
        "\n",
        "# Additional parameters\n",
        "gamma = 0.0\n",
        "delta = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qQJ3Ep6ljwO"
      },
      "source": [
        "Count the words of the smallest and largest abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xEhbCPl7UQY",
        "outputId": "cb348bd4-2dac-4669-932a-39558ba0ca57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max word count: 473\n",
            "Min word count: 10\n"
          ]
        }
      ],
      "source": [
        "max_word_count = 0\n",
        "min_word_count = float('inf')\n",
        "\n",
        "with open(f'/content/datasets/{datasetName}/{dataTextFile}', 'r') as file:\n",
        "    for line in file:\n",
        "        word_count = len(line.split())\n",
        "\n",
        "        if word_count > max_word_count:\n",
        "            max_word_count = word_count\n",
        "\n",
        "        if word_count < min_word_count:\n",
        "            min_word_count = word_count\n",
        "\n",
        "print(\"Max word count:\", max_word_count)\n",
        "print(\"Min word count:\", min_word_count)\n",
        "\n",
        "\n",
        "MAX_LEN = max_word_count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQf-Hk3EljwP"
      },
      "source": [
        "Execute the code below if the data file with the astracts is too large and needs extracting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdXCdfP_-hHD",
        "outputId": "8362d0d7-ea0c-4ce5-fde7-d5c44a838a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete!\n"
          ]
        }
      ],
      "source": [
        "zip_file_path = '/content/data.zip'\n",
        "extract_to = f'/content/datasets/{datasetName}'\n",
        "\n",
        "# Open and extract the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(\"Extraction complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcWCLJQxVIlg"
      },
      "outputs": [],
      "source": [
        "def sub_Mat(P, node):\n",
        "\n",
        "    sub_P = np.zeros((len(node),len(node)))\n",
        "    for i in range(len(node)):\n",
        "        for j in range(len(node)):\n",
        "            sub_P[i,j] = P[node[i],node[j]]\n",
        "\n",
        "    return sub_P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwNH2N0iVJ1Y"
      },
      "source": [
        "# ***DataSet***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KcJkcNBs_G3"
      },
      "outputs": [],
      "source": [
        "class dataSet:\n",
        "    def __init__(self, text_path, graph_path, labels_path=None):\n",
        "\n",
        "        #text_file, graph_file, labels_file = self.load(text_path, graph_path, labels_path)\n",
        "        text_file, graph_file = self.load(text_path, graph_path)\n",
        "\n",
        "        self.edges = self.load_edges(graph_file)\n",
        "\n",
        "        #self.labels = self.load_labels(labels_file)\n",
        "\n",
        "        self.text, self.num_vocab, self.num_nodes = self.load_text(text_file)\n",
        "\n",
        "        self.nodes = range(0, self.num_nodes)\n",
        "\n",
        "        self.negative_table = InitNegTable(self.edges)\n",
        "\n",
        "        self.P = self.P_matrix(self.edges, self.num_nodes)\n",
        "\n",
        "    \"\"\"\n",
        "    def load(self, text_path, graph_path, labels_path):\n",
        "        text_file = open(text_path, 'rb').readlines()\n",
        "        graph_file = open(graph_path, 'rb').readlines()\n",
        "        labels_file = open(labels_path, 'rb').readlines()\n",
        "\n",
        "        return text_file, graph_file, label_file\n",
        "    \"\"\"\n",
        "\n",
        "    def load(self, text_path, graph_path):\n",
        "        text_file = open(text_path, 'rb').readlines()\n",
        "        graph_file = open(graph_path, 'rb').readlines()\n",
        "\n",
        "        return text_file, graph_file\n",
        "\n",
        "    def load_edges(self, graph_file):\n",
        "        edges = []\n",
        "        for i in graph_file:\n",
        "            if np.random.uniform(0.0, 1.0) <= ratio:\n",
        "                edges.append(list(map(int, i.strip().decode().split('\\t'))))\n",
        "\n",
        "        return edges\n",
        "\n",
        "    \"\"\"\n",
        "    def load_labels(self, labels_file):\n",
        "      labels = []\n",
        "      for i in labels_file:\n",
        "        labels.append(list(map(int, i.strip().decode().split(','))))\n",
        "\n",
        "      return labels\n",
        "\n",
        "    def adj_list(self, edges):\n",
        "      node1, node2 = zip(*edges)\n",
        "\n",
        "      # Create adjacency list\n",
        "      adj_list = defaultdict(list)\n",
        "      for n1, n2 in edges:\n",
        "          adj_list[n1].append(n2)\n",
        "          adj_list[n2].append(n1)\n",
        "\n",
        "      return dict(adj_list)\n",
        "    \"\"\"\n",
        "\n",
        "    # This method has been modified to be compatible with newer versions\n",
        "    def load_text(self, text_file):\n",
        "        \"\"\"\n",
        "        Adapting with adapt(text_data):\n",
        "\n",
        "        vectorize_layer.adapt(text_data) analyzes text_data, builds a vocabulary, and assigns a unique integer ID to each word based on its frequency (most frequent words get lower IDs).\n",
        "        Transforming with vectorize_layer(text_data):\n",
        "\n",
        "        This maps each word in text_data to its corresponding integer token ID, producing a 2D array where each row represents a sequence of token IDs for a given input line, padded or truncated to max_len.\n",
        "        \"\"\"\n",
        "        vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "            max_tokens=None,  # Set a limit if needed\n",
        "            output_mode='int',\n",
        "            output_sequence_length=MAX_LEN\n",
        "        )\n",
        "\n",
        "        text_data = [line.strip() for line in text_file]\n",
        "\n",
        "        vectorize_layer.adapt(text_data)\n",
        "\n",
        "        text = vectorize_layer(text_data).numpy()\n",
        "\n",
        "        num_vocab = len(vectorize_layer.get_vocabulary())\n",
        "        print(f'Vocabulary: {num_vocab}')\n",
        "        num_nodes = len(text)\n",
        "\n",
        "        return text, num_vocab, num_nodes\n",
        "\n",
        "    def negative_sample(self, edges):\n",
        "        # edges is the sample_edges in self.generate_batches()\n",
        "        node1, node2 = zip(*edges)\n",
        "        sample_edges = []\n",
        "\n",
        "        # The negative table contains edges that don not exist\n",
        "        func = lambda: self.negative_table[random.randint(0, neg_table_size - 1)] # Pick a random node from the negative table\n",
        "\n",
        "        # For each edge...\n",
        "        for i in range(len(edges)):\n",
        "            neg_node = func() # Pick a negative node\n",
        "\n",
        "            # If the negative node is identical to the first and second node in the current edge...\n",
        "            while node1[i] == neg_node or node2[i] == neg_node:\n",
        "                neg_node = func() # Pick another negative node until the neg node is different than the first and second node in the current edge\n",
        "\n",
        "            # Create a new type of edge that has an additional node, the negative node\n",
        "            sample_edges.append([node1[i], node2[i], neg_node])\n",
        "\n",
        "        return sample_edges\n",
        "\n",
        "    def generate_batches(self, mode=None):\n",
        "\n",
        "        num_batch = len(self.edges) // batch_size\n",
        "        edges = self.edges\n",
        "        if mode == 'add':\n",
        "            num_batch += 1\n",
        "            edges.extend(edges[:(batch_size - len(self.edges) % batch_size)])\n",
        "        if mode != 'add':\n",
        "            random.shuffle(edges)\n",
        "        sample_edges = edges[:num_batch * batch_size]\n",
        "\n",
        "        # For each edge in \"sample_edges\", add a negative edge\n",
        "        sample_edges = self.negative_sample(sample_edges)\n",
        "\n",
        "        # Create batches of edges\n",
        "\n",
        "        \"\"\"\n",
        "        The first batch range is 0 -- batch_size - 1\n",
        "        The second batch range is batch_size -- 2 * batch_size - 1\n",
        "        The third batch range is 2 * batch_size -- 3* batch_size - 1 and so on\n",
        "        \"\"\"\n",
        "        batches = []\n",
        "        for i in range(num_batch):\n",
        "            batches.append(sample_edges[i * batch_size:(i + 1) * batch_size])\n",
        "\n",
        "        return batches\n",
        "\n",
        "    def nodes_batches(self, mode=None):\n",
        "\n",
        "        num_batch = len(self.nodes) // batch_size\n",
        "        nodes = self.nodes\n",
        "        if mode == 'add':\n",
        "            num_batch += 1\n",
        "            nodes.extend(nodes[:(batch_size - len(self.nodes) % batch_size)])\n",
        "            random.shuffle(nodes)\n",
        "        if mode != 'add':\n",
        "            random.shuffle(nodes)\n",
        "        sample_nodes = nodes[:num_batch * batch_size]\n",
        "\n",
        "        batches = []\n",
        "        for i in range(num_batch):\n",
        "            batches.append(sample_nodes[i * batch_size:(i + 1) * batch_size])\n",
        "        return batches\n",
        "\n",
        "    def P_matrix(self, edges, num_nodes):\n",
        "        # Take all the edges\n",
        "        a_list,b_list=zip(*edges)\n",
        "        a_list=list(a_list) # This list contains the first nodes in all edges\n",
        "        b_list=list(b_list) # This list contains the second nodes in all edges\n",
        "\n",
        "        P = np.zeros((num_nodes,num_nodes))\n",
        "\n",
        "        for i in range(len(a_list)):\n",
        "            P[a_list[i],b_list[i]]=1 # The prob of transitioning from \"a_list[i]\" to \"b_list[i]\". Initially it's 1 for unweighted graphs\n",
        "            P[b_list[i],a_list[i]]=1 # The prob of transitioning from \"b_list[i]\" to \"a_list[i]\". Initially it's 1 for unweighted graphs\n",
        "\n",
        "        P = normalize(P, axis=1, norm='l1') # We normalize P\n",
        "\n",
        "        return P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AV9wwI_7NSC"
      },
      "source": [
        "# ***DMTE***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S23Tcvj9tQnb"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, vocab_size, num_nodes, alpha, beta, num_labels=None):\n",
        "        # '''hyperparameter'''\n",
        "        with tf.name_scope('read_inputs') as scope:\n",
        "            self.Text_a = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Ta')\n",
        "            self.Text_b = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tb')\n",
        "            self.Text_neg = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tneg')\n",
        "            self.Node_a = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n1')\n",
        "            self.Node_b = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n2')\n",
        "            self.Node_neg = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n3')\n",
        "            self.P_a = tf.compat.v1.placeholder(tf.float32, [batch_size, batch_size], name='Pa')\n",
        "            self.P_b = tf.compat.v1.placeholder(tf.float32, [batch_size, batch_size], name='Pb')\n",
        "            self.P_neg = tf.compat.v1.placeholder(tf.float32, [batch_size, batch_size], name='Pneg')\n",
        "\n",
        "        with tf.name_scope('initialize_embedding') as scope:\n",
        "            self.text_embed = tf.Variable(tf.random.truncated_normal([vocab_size, word_embed_size], stddev=0.3))\n",
        "            self.node_embed = tf.Variable(tf.random.truncated_normal([num_nodes, embed_size // 2], stddev=0.3))\n",
        "            self.node_embed = tf.clip_by_norm(self.node_embed, clip_norm=1, axes=1)\n",
        "\n",
        "        with tf.name_scope('lookup_embeddings') as scope:\n",
        "            self.TA = tf.nn.embedding_lookup(self.text_embed, self.Text_a)\n",
        "            self.T_A = tf.expand_dims(self.TA, -1)\n",
        "\n",
        "            self.TB = tf.nn.embedding_lookup(self.text_embed, self.Text_b)\n",
        "            self.T_B = tf.expand_dims(self.TB, -1)\n",
        "\n",
        "            self.TNEG = tf.nn.embedding_lookup(self.text_embed, self.Text_neg)\n",
        "            self.T_NEG = tf.expand_dims(self.TNEG, -1)\n",
        "\n",
        "            self.N_A = tf.nn.embedding_lookup(self.node_embed, self.Node_a)\n",
        "            self.N_B = tf.nn.embedding_lookup(self.node_embed, self.Node_b)\n",
        "            self.N_NEG = tf.nn.embedding_lookup(self.node_embed, self.Node_neg)\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.convA, self.convB, self.convNeg = self.conv()\n",
        "        self.loss = self.compute_loss()\n",
        "\n",
        "    def conv(self):\n",
        "\n",
        "        W0 = tf.Variable(tf.random.truncated_normal([word_embed_size, embed_size // 2], stddev=0.3))\n",
        "        W1 = tf.Variable(tf.random.truncated_normal([word_embed_size, embed_size // 2], stddev=0.3))\n",
        "        W2 = tf.Variable(tf.random.truncated_normal([word_embed_size, embed_size // 2], stddev=0.3))\n",
        "\n",
        "        # Additional weight matrices\n",
        "        #W3 = tf.Variable(tf.random.truncated_normal([word_embed_size, embed_size // 2], stddev=0.3))\n",
        "        #W4 = tf.Variable(tf.random.truncated_normal([word_embed_size, embed_size // 2], stddev=0.3))\n",
        "\n",
        "        mA = tf.reduce_mean(self.T_A, axis=1, keepdims=True)\n",
        "        mB = tf.reduce_mean(self.T_B, axis=1, keepdims=True)\n",
        "        mNEG = tf.reduce_mean(self.T_NEG, axis=1, keepdims=True)\n",
        "\n",
        "        convA = tf.tanh(tf.squeeze(mA))\n",
        "        convB = tf.tanh(tf.squeeze(mB))\n",
        "        convNEG = tf.tanh(tf.squeeze(mNEG))\n",
        "\n",
        "        attA = (tf.matmul(convA, W0) +\n",
        "                self.alpha * tf.matmul(tf.matmul(self.P_a, convA), W1) +\n",
        "                self.beta * tf.matmul(tf.matmul(tf.math.square(self.P_a), convA), W2))\n",
        "                #gamma * tf.matmul(tf.matmul(tf.math.pow(self.P_a, 3), convA), W3) +\n",
        "                #delta * tf.matmul(tf.matmul(tf.math.pow(self.P_a, 4), convA), W4))\n",
        "\n",
        "        attB = (tf.matmul(convB, W0) +\n",
        "                self.alpha * tf.matmul(tf.matmul(self.P_b, convB), W1) +\n",
        "                self.beta * tf.matmul(tf.matmul(tf.math.square(self.P_b), convB), W2))\n",
        "                #gamma * tf.matmul(tf.matmul(tf.math.pow(self.P_b, 3), convB), W3) +\n",
        "                #delta * tf.matmul(tf.matmul(tf.math.pow(self.P_b, 4), convB), W4))\n",
        "\n",
        "\n",
        "        attNEG = (tf.matmul(convNEG, W0) +\n",
        "                  self.alpha * tf.matmul(tf.matmul(self.P_a, convNEG), W1) +\n",
        "                  self.beta * tf.matmul(tf.matmul(tf.math.square(self.P_a), convNEG), W2))\n",
        "                  #gamma * tf.matmul(tf.matmul(tf.math.pow(self.P_a, 3), convNEG), W3) +\n",
        "                  #delta * tf.matmul(tf.matmul(tf.math.pow(self.P_a, 4), convNEG), W4))\n",
        "\n",
        "        return attA, attB, attNEG\n",
        "\n",
        "    def compute_loss(self):\n",
        "\n",
        "        # Loss functions for:\n",
        "\n",
        "\n",
        "        # Text-Text\n",
        "        p1 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.convA, self.convB), 1)) + 0.001)\n",
        "\n",
        "        p2 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.convA, self.convNeg), 1)) + 0.001)\n",
        "\n",
        "        p11 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.convB, self.convA), 1)) + 0.001)\n",
        "\n",
        "        p12 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.convB, self.convNeg), 1)) + 0.001)\n",
        "\n",
        "\n",
        "\n",
        "        # Node-Node\n",
        "        p3 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.N_A +\n",
        "                                                                 self.alpha * tf.matmul(self.P_a, self.N_A) +\n",
        "                                                                 self.beta * tf.matmul(tf.math.square(self.P_a), self.N_A), self.N_B), 1)) + 0.001)\n",
        "                                                                 #gamma * tf.matmul(tf.math.pow(self.P_a, 3), self.N_A) +\n",
        "                                                                 #delta * tf.matmul(tf.math.pow(self.P_a, 4), self.N_A), self.N_B), 1)) + 0.001)\n",
        "\n",
        "        p4 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.N_A +\n",
        "                                                                  self.alpha * tf.matmul(self.P_a, self.N_A) +\n",
        "                                                                  self.beta * tf.matmul(tf.math.square(self.P_a), self.N_A), self.N_NEG), 1)) + 0.001)\n",
        "                                                                  #gamma * tf.matmul(tf.math.pow(self.P_a, 3), self.N_A) +\n",
        "                                                                  #delta * tf.matmul(tf.math.pow(self.P_a, 4), self.N_A), self.N_NEG), 1)) + 0.001)\n",
        "\n",
        "        p13 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.N_B +\n",
        "                                                                  self.alpha * tf.matmul(self.P_b, self.N_B) +\n",
        "                                                                  self.beta * tf.matmul(tf.math.square(self.P_b), self.N_B), self.N_A), 1)) + 0.001)\n",
        "                                                                  #gamma * tf.matmul(tf.math.pow(self.P_b, 3), self.N_B) +\n",
        "                                                                  #delta * tf.matmul(tf.math.pow(self.P_b, 4), self.N_B), self.N_A), 1)) + 0.001)\n",
        "\n",
        "        p14 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.N_B +\n",
        "                                                                   self.alpha * tf.matmul(self.P_b, self.N_B) +\n",
        "                                                                   self.beta * tf.matmul(tf.math.square(self.P_b), self.N_B), self.N_NEG), 1)) + 0.001)\n",
        "                                                                   #gamma * tf.matmul(tf.math.pow(self.P_b, 3), self.N_B) +\n",
        "                                                                   #delta * tf.matmul(tf.math.pow(self.P_b, 4), self.N_B), self.N_NEG), 1)) + 0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Node-Text\n",
        "        p5 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.N_A +\n",
        "                                                                 self.alpha * tf.matmul(self.P_a, self.N_A) +\n",
        "                                                                 self.beta * tf.matmul(tf.math.square(self.P_a), self.N_A), self.convB), 1)) + 0.001)\n",
        "                                                                 #gamma * tf.matmul(tf.math.pow(self.P_a, 3), self.N_A) +\n",
        "                                                                 #delta * tf.matmul(tf.math.pow(self.P_a, 4), self.N_A), self.convB), 1)) + 0.001)\n",
        "\n",
        "        p6 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.N_A +\n",
        "                                                                  self.alpha * tf.matmul(self.P_a, self.N_A) +\n",
        "                                                                  self.beta * tf.matmul(tf.math.square(self.P_a), self.N_A), self.convNeg), 1)) + 0.001)\n",
        "                                                                  #gamma * tf.matmul(tf.math.pow(self.P_a, 3), self.N_A) +\n",
        "                                                                  #delta * tf.matmul(tf.math.pow(self.P_a, 4), self.N_A), self.convNeg), 1)) + 0.001)\n",
        "\n",
        "        p15 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.N_B +\n",
        "                                                                  self.alpha * tf.matmul(self.P_b, self.N_B) +\n",
        "                                                                  self.beta * tf.matmul(tf.math.square(self.P_b), self.N_B), self.convA), 1)) + 0.001)\n",
        "                                                                  #gamma * tf.matmul(tf.math.pow(self.P_b, 3), self.N_B) +\n",
        "                                                                  #delta * tf.matmul(tf.math.pow(self.P_b, 4), self.N_B), self.convA), 1)) + 0.001)\n",
        "\n",
        "        p16 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.N_B +\n",
        "                                                                   self.alpha * tf.matmul(self.P_b, self.N_B) +\n",
        "                                                                   self.beta * tf.matmul(tf.math.square(self.P_b), self.N_B), self.convNeg), 1)) + 0.001)\n",
        "                                                                   #gamma * tf.matmul(tf.math.pow(self.P_b, 3), self.N_B) +\n",
        "                                                                   #delta * tf.matmul(tf.math.pow(self.P_b, 4), self.N_B), self.convNeg), 1)) + 0.001)\n",
        "\n",
        "\n",
        "\n",
        "        # Text-Node\n",
        "        p7 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.convA, self.N_B), 1)) + 0.001)\n",
        "\n",
        "        p8 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.convA, self.N_NEG), 1)) + 0.001)\n",
        "\n",
        "        p17 = tf.math.log(tf.nn.sigmoid(tf.reduce_sum(tf.multiply(self.convB, self.N_A), 1)) + 0.001)\n",
        "\n",
        "        p18 = tf.math.log(tf.nn.sigmoid(-tf.reduce_sum(tf.multiply(self.convB, self.N_NEG), 1)) + 0.001)\n",
        "\n",
        "        rho1 = 1.0\n",
        "        rho2 = 1.0\n",
        "        rho3 = 0.3\n",
        "        temp_loss = rho1 * (p1 + p2 + p11 + p12) + rho2 * (p3 + p4 + p13 + p14) + rho3 * (p5 + p6 + p15 + p16) + rho3 * (p7 + p8 + p17 + p18)\n",
        "        loss = -tf.reduce_sum(temp_loss)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vt_wvryq0oh"
      },
      "source": [
        "# ***Negative Sample***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdwgdktTq504"
      },
      "outputs": [],
      "source": [
        "def InitNegTable(edges):\n",
        "    a_list, b_list = zip(*edges)\n",
        "    a_list = list(a_list)\n",
        "    b_list = list(b_list)\n",
        "    node = a_list\n",
        "    node.extend(b_list)\n",
        "\n",
        "    node_degree = {}\n",
        "    for i in node:\n",
        "        if i in node_degree:\n",
        "            node_degree[i] += 1\n",
        "        else:\n",
        "            node_degree[i] = 1\n",
        "    sum_degree = 0\n",
        "    for i in node_degree.values():\n",
        "        sum_degree += pow(i, 0.75)\n",
        "\n",
        "    por = 0\n",
        "    cur_sum = 0\n",
        "    vid = -1\n",
        "    neg_table = []\n",
        "    degree_list = list(node_degree.values())\n",
        "    node_id = list(node_degree.keys())\n",
        "    for i in range(neg_table_size):\n",
        "        if ((i + 1) / float(neg_table_size)) > por:\n",
        "            cur_sum += pow(degree_list[vid + 1], NEG_SAMPLE_POWER)\n",
        "            por = cur_sum / sum_degree\n",
        "            vid += 1\n",
        "        neg_table.append(node_id[vid])\n",
        "    print(f'Neg. table size: {len(neg_table)}')\n",
        "    return neg_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLaqva64WoA0"
      },
      "source": [
        "# ***Run (Single execution)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgbe18H4WsgX"
      },
      "outputs": [],
      "source": [
        "def prepareData(datasetName, ratio):\n",
        "  f = open('/content/datasets/%s/graph.txt' % datasetName, 'rb')\n",
        "  edges = [i for i in f]\n",
        "  selected = int(len(edges) * float(ratio))\n",
        "  selected = selected - selected % batch_size\n",
        "  selected = random.sample(edges, selected)\n",
        "  remain = [i for i in edges if i not in selected]\n",
        "  try:\n",
        "    temp_dir = Path('temp')\n",
        "\n",
        "    # Check if the directory exists, if so, delete it\n",
        "    if temp_dir.exists() and temp_dir.is_dir():\n",
        "        shutil.rmtree(temp_dir)\n",
        "        print(\"Existing directory deleted.\")\n",
        "\n",
        "    # Create the directory\n",
        "    temp_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(\"Directory created successfully.\")\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "  fw1 = open('temp/graph.txt', 'wb')\n",
        "  fw2 = open('temp/test_graph.txt', 'wb')\n",
        "\n",
        "  for i in selected:\n",
        "      fw1.write(i)\n",
        "  for i in remain:\n",
        "      fw2.write(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK_Z-E1HXRVm",
        "outputId": "afc9951e-bf8d-4d11-c35d-08f6ce598c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory created successfully.\n"
          ]
        }
      ],
      "source": [
        "prepareData(datasetName, ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wFgChvWggiu",
        "outputId": "471f54d0-57f0-41fc-d824-495008c83452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: 14696\n",
            "Neg. table size: 1000000\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "dataset_name = datasetName\n",
        "graph_path = os.path.join('/content/temp/graph.txt')\n",
        "text_path = os.path.join(\"/content\", \"datasets\", dataset_name, dataTextFile)\n",
        "\n",
        "data = dataSet(text_path, graph_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QsRmNqLXrGb",
        "outputId": "db306b08-2246-4ffa-ad46-2e94f428154d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training.......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:   2%|▎         | 5/200 [00:03<01:49,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  1  loss:  1173.5244140625\n",
            "epoch:  2  loss:  1174.24072265625\n",
            "epoch:  3  loss:  1154.207275390625\n",
            "epoch:  4  loss:  1172.4483642578125\n",
            "epoch:  5  loss:  1152.0401611328125\n",
            "epoch:  6  loss:  1151.056884765625\n",
            "epoch:  7  loss:  1164.86083984375\n",
            "epoch:  8  loss:  1154.0650634765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:   7%|▋         | 14/200 [00:03<00:27,  6.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  9  loss:  1151.578857421875\n",
            "epoch:  10  loss:  1158.53173828125\n",
            "epoch:  11  loss:  1162.73046875\n",
            "epoch:  12  loss:  1123.0736083984375\n",
            "epoch:  13  loss:  1146.65771484375\n",
            "epoch:  14  loss:  1124.2763671875\n",
            "epoch:  15  loss:  1144.7598876953125\n",
            "epoch:  16  loss:  1116.37841796875\n",
            "epoch:  17  loss:  1111.78955078125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  12%|█▏        | 23/200 [00:04<00:12, 13.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  18  loss:  1087.02685546875\n",
            "epoch:  19  loss:  1078.768798828125\n",
            "epoch:  20  loss:  985.6742553710938\n",
            "epoch:  21  loss:  953.3301391601562\n",
            "epoch:  22  loss:  923.7408447265625\n",
            "epoch:  23  loss:  797.2402954101562\n",
            "epoch:  24  loss:  721.5189208984375\n",
            "epoch:  25  loss:  665.926025390625\n",
            "epoch:  26  loss:  589.0726928710938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  16%|█▌        | 32/200 [00:04<00:07, 21.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  27  loss:  541.81591796875\n",
            "epoch:  28  loss:  518.4888916015625\n",
            "epoch:  29  loss:  487.5359191894531\n",
            "epoch:  30  loss:  478.1725158691406\n",
            "epoch:  31  loss:  462.22637939453125\n",
            "epoch:  32  loss:  457.2728271484375\n",
            "epoch:  33  loss:  456.0660400390625\n",
            "epoch:  34  loss:  453.733154296875\n",
            "epoch:  35  loss:  458.64306640625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  20%|██        | 40/200 [00:04<00:05, 28.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  36  loss:  457.6021728515625\n",
            "epoch:  37  loss:  457.1626892089844\n",
            "epoch:  38  loss:  453.93988037109375\n",
            "epoch:  39  loss:  457.95416259765625\n",
            "epoch:  40  loss:  454.21343994140625\n",
            "epoch:  41  loss:  456.1550598144531\n",
            "epoch:  42  loss:  454.9295959472656\n",
            "epoch:  43  loss:  459.04974365234375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  24%|██▍       | 48/200 [00:04<00:04, 32.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  44  loss:  454.1771545410156\n",
            "epoch:  45  loss:  454.8286437988281\n",
            "epoch:  46  loss:  451.9372863769531\n",
            "epoch:  47  loss:  451.1806640625\n",
            "epoch:  48  loss:  450.6763916015625\n",
            "epoch:  49  loss:  447.9947509765625\n",
            "epoch:  50  loss:  447.8292236328125\n",
            "epoch:  51  loss:  446.8936767578125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  28%|██▊       | 57/200 [00:05<00:03, 38.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  52  loss:  445.7368469238281\n",
            "epoch:  53  loss:  442.2054443359375\n",
            "epoch:  54  loss:  443.56732177734375\n",
            "epoch:  55  loss:  440.69512939453125\n",
            "epoch:  56  loss:  442.2057189941406\n",
            "epoch:  57  loss:  440.3218994140625\n",
            "epoch:  58  loss:  438.1889953613281\n",
            "epoch:  59  loss:  439.9997863769531\n",
            "epoch:  60  loss:  437.0\n",
            "epoch:  61  loss:  437.03546142578125\n",
            "epoch:  62  loss:  436.3720397949219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  34%|███▍      | 69/200 [00:05<00:02, 47.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  63  loss:  437.1236267089844\n",
            "epoch:  64  loss:  437.3717346191406\n",
            "epoch:  65  loss:  437.6768798828125\n",
            "epoch:  66  loss:  436.0179138183594\n",
            "epoch:  67  loss:  434.22137451171875\n",
            "epoch:  68  loss:  433.870849609375\n",
            "epoch:  69  loss:  434.51593017578125\n",
            "epoch:  70  loss:  433.2633056640625\n",
            "epoch:  71  loss:  432.7579650878906\n",
            "epoch:  72  loss:  432.4459533691406\n",
            "epoch:  73  loss:  431.6860656738281\n",
            "epoch:  74  loss:  429.9345397949219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  41%|████      | 82/200 [00:05<00:02, 54.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  75  loss:  427.79095458984375\n",
            "epoch:  76  loss:  431.9444580078125\n",
            "epoch:  77  loss:  430.1364440917969\n",
            "epoch:  78  loss:  429.7733154296875\n",
            "epoch:  79  loss:  431.08953857421875\n",
            "epoch:  80  loss:  430.47845458984375\n",
            "epoch:  81  loss:  430.384521484375\n",
            "epoch:  82  loss:  427.6012878417969\n",
            "epoch:  83  loss:  425.7528381347656\n",
            "epoch:  84  loss:  426.482666015625\n",
            "epoch:  85  loss:  424.9984130859375\n",
            "epoch:  86  loss:  424.6258850097656\n",
            "epoch:  87  loss:  423.8175048828125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  48%|████▊     | 96/200 [00:05<00:01, 58.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  88  loss:  424.8148498535156\n",
            "epoch:  89  loss:  424.79217529296875\n",
            "epoch:  90  loss:  419.984375\n",
            "epoch:  91  loss:  421.1540222167969\n",
            "epoch:  92  loss:  422.2255554199219\n",
            "epoch:  93  loss:  422.36376953125\n",
            "epoch:  94  loss:  420.3765869140625\n",
            "epoch:  95  loss:  419.77191162109375\n",
            "epoch:  96  loss:  416.397216796875\n",
            "epoch:  97  loss:  421.25640869140625\n",
            "epoch:  98  loss:  417.799072265625\n",
            "epoch:  99  loss:  417.11053466796875\n",
            "epoch:  100  loss:  419.8067321777344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  55%|█████▍    | 109/200 [00:05<00:01, 56.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  101  loss:  419.2234191894531\n",
            "epoch:  102  loss:  421.1321716308594\n",
            "epoch:  103  loss:  416.65020751953125\n",
            "epoch:  104  loss:  415.495849609375\n",
            "epoch:  105  loss:  414.22412109375\n",
            "epoch:  106  loss:  413.58001708984375\n",
            "epoch:  107  loss:  414.04736328125\n",
            "epoch:  108  loss:  413.18243408203125\n",
            "epoch:  109  loss:  413.91009521484375\n",
            "epoch:  110  loss:  412.515869140625\n",
            "epoch:  111  loss:  412.89642333984375\n",
            "epoch:  112  loss:  413.33819580078125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  62%|██████▏   | 123/200 [00:06<00:01, 59.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  113  loss:  410.89801025390625\n",
            "epoch:  114  loss:  414.0845947265625\n",
            "epoch:  115  loss:  407.47027587890625\n",
            "epoch:  116  loss:  412.28851318359375\n",
            "epoch:  117  loss:  408.349853515625\n",
            "epoch:  118  loss:  410.3521728515625\n",
            "epoch:  119  loss:  406.89605712890625\n",
            "epoch:  120  loss:  407.666259765625\n",
            "epoch:  121  loss:  407.37847900390625\n",
            "epoch:  122  loss:  404.1093444824219\n",
            "epoch:  123  loss:  406.41461181640625\n",
            "epoch:  124  loss:  404.8984375\n",
            "epoch:  125  loss:  405.539794921875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  68%|██████▊   | 137/200 [00:06<00:01, 61.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  126  loss:  407.5130310058594\n",
            "epoch:  127  loss:  407.4278564453125\n",
            "epoch:  128  loss:  402.0755615234375\n",
            "epoch:  129  loss:  402.1451721191406\n",
            "epoch:  130  loss:  402.386474609375\n",
            "epoch:  131  loss:  405.47869873046875\n",
            "epoch:  132  loss:  394.49072265625\n",
            "epoch:  133  loss:  398.353759765625\n",
            "epoch:  134  loss:  400.8099670410156\n",
            "epoch:  135  loss:  396.1330261230469\n",
            "epoch:  136  loss:  394.53656005859375\n",
            "epoch:  137  loss:  397.3466796875\n",
            "epoch:  138  loss:  397.70733642578125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  76%|███████▌  | 151/200 [00:06<00:00, 60.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  139  loss:  395.216064453125\n",
            "epoch:  140  loss:  398.314453125\n",
            "epoch:  141  loss:  395.197265625\n",
            "epoch:  142  loss:  393.09503173828125\n",
            "epoch:  143  loss:  397.183349609375\n",
            "epoch:  144  loss:  393.8685607910156\n",
            "epoch:  145  loss:  391.1815185546875\n",
            "epoch:  146  loss:  392.2414245605469\n",
            "epoch:  147  loss:  393.87994384765625\n",
            "epoch:  148  loss:  390.54718017578125\n",
            "epoch:  149  loss:  387.286865234375\n",
            "epoch:  150  loss:  389.736083984375\n",
            "epoch:  151  loss:  388.5830383300781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:  79%|███████▉  | 158/200 [00:06<00:00, 59.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  152  loss:  389.41644287109375\n",
            "epoch:  153  loss:  384.4251708984375\n",
            "epoch:  154  loss:  384.6579284667969\n",
            "epoch:  155  loss:  385.1009521484375\n",
            "epoch:  156  loss:  383.4337158203125\n",
            "epoch:  157  loss:  384.91632080078125\n",
            "epoch:  158  loss:  388.1527099609375\n",
            "epoch:  159  loss:  383.04254150390625\n",
            "epoch:  160  loss:  383.5155334472656\n",
            "epoch:  161  loss:  388.5513000488281\n",
            "epoch:  162  loss:  380.39703369140625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  86%|████████▌ | 171/200 [00:06<00:00, 57.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  163  loss:  374.1988525390625\n",
            "epoch:  164  loss:  378.568115234375\n",
            "epoch:  165  loss:  377.6250915527344\n",
            "epoch:  166  loss:  380.1463623046875\n",
            "epoch:  167  loss:  375.5496826171875\n",
            "epoch:  168  loss:  380.25164794921875\n",
            "epoch:  169  loss:  378.221435546875\n",
            "epoch:  170  loss:  381.31927490234375\n",
            "epoch:  171  loss:  377.1251525878906\n",
            "epoch:  172  loss:  379.8698425292969\n",
            "epoch:  173  loss:  379.19085693359375\n",
            "epoch:  174  loss:  377.0843200683594\n",
            "epoch:  175  loss:  376.36199951171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  92%|█████████▏| 184/200 [00:07<00:00, 59.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  176  loss:  382.3316955566406\n",
            "epoch:  177  loss:  378.7127685546875\n",
            "epoch:  178  loss:  378.74945068359375\n",
            "epoch:  179  loss:  373.7747802734375\n",
            "epoch:  180  loss:  373.53643798828125\n",
            "epoch:  181  loss:  374.49932861328125\n",
            "epoch:  182  loss:  372.39154052734375\n",
            "epoch:  183  loss:  366.13671875\n",
            "epoch:  184  loss:  365.71014404296875\n",
            "epoch:  185  loss:  364.03631591796875\n",
            "epoch:  186  loss:  366.6719970703125\n",
            "epoch:  187  loss:  362.9066467285156\n",
            "epoch:  188  loss:  360.530029296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs: 100%|██████████| 200/200 [00:07<00:00, 26.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  189  loss:  372.97332763671875\n",
            "epoch:  190  loss:  374.51824951171875\n",
            "epoch:  191  loss:  358.698486328125\n",
            "epoch:  192  loss:  366.5995178222656\n",
            "epoch:  193  loss:  373.2900695800781\n",
            "epoch:  194  loss:  373.1017761230469\n",
            "epoch:  195  loss:  367.3129577636719\n",
            "epoch:  196  loss:  370.6121826171875\n",
            "epoch:  197  loss:  370.6998291015625\n",
            "epoch:  198  loss:  361.46771240234375\n",
            "epoch:  199  loss:  351.54949951171875\n",
            "epoch:  200  loss:  360.9666748046875\n",
            "Time: 7.201402000000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "with tf.Graph().as_default():\n",
        "    sess = tf.compat.v1.Session()\n",
        "    with sess.as_default():\n",
        "        model = Model(data.num_vocab, data.num_nodes, alpha, beta)\n",
        "        opt = tf.compat.v1.train.AdamOptimizer(lr)#tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        train_op = opt.minimize(model.loss)#opt.minimize(model.loss, var_list=model.trainable_variables)\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        time = 0\n",
        "\n",
        "        # training\n",
        "        print('start training.......')\n",
        "\n",
        "\n",
        "        for epoch in tqdm(range(num_epoch), desc=\"Epochs\"):\n",
        "            start = datetime.now()\n",
        "            loss_epoch = 0\n",
        "            batches = data.generate_batches()\n",
        "            h1 = 0\n",
        "            num_batch = len(batches)\n",
        "            for i in range(num_batch):\n",
        "                batch = batches[i]\n",
        "\n",
        "                node1, node2, node3 = zip(*batch)\n",
        "                node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "                #labels1, labels2 = data.labels[node1], data.labels[node2]\n",
        "                #labels = [max(a, b) for a, b in zip(labels1, labels2)]\n",
        "                P1, P2, P3 = sub_Mat(data.P, node1), sub_Mat(data.P, node2), sub_Mat(data.P, node3)\n",
        "\n",
        "\n",
        "                feed_dict = {\n",
        "                    model.Text_a: text1,\n",
        "                    model.Text_b: text2,\n",
        "                    model.Text_neg: text3,\n",
        "                    model.Node_a: node1,\n",
        "                    model.Node_b: node2,\n",
        "                    model.Node_neg: node3,\n",
        "                    model.P_a: P1,\n",
        "                    model.P_b: P2,\n",
        "                    model.P_neg: P3\n",
        "                    #model.labels: labels\n",
        "                }\n",
        "\n",
        "                # run the graph\n",
        "                _, loss_batch = sess.run([train_op, model.loss], feed_dict=feed_dict)\n",
        "\n",
        "                loss_epoch += loss_batch\n",
        "\n",
        "            end = datetime.now()\n",
        "            time += (end - start).total_seconds()\n",
        "            print('epoch: ', epoch + 1, ' loss: ', loss_epoch)\n",
        "\n",
        "        print(f'Time: {time}')\n",
        "        # Saving embeddings\n",
        "        with open('temp/embed.txt', 'wb') as file:\n",
        "            batches = data.generate_batches(mode='add')\n",
        "            num_batch = len(batches)\n",
        "            embed = [[] for _ in range(data.num_nodes)]\n",
        "\n",
        "            for i in range(num_batch):\n",
        "                batch = batches[i]\n",
        "                node1, node2, node3 = zip(*batch)\n",
        "                node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "                #labels1, labels2 = data.labels[node1], data.labels[node2]\n",
        "                #labels = [max(a, b) for a, b in zip(labels1, labels2)]\n",
        "                P1, P2, P3 = sub_Mat(data.P, node1), sub_Mat(data.P, node2), sub_Mat(data.P, node3)\n",
        "\n",
        "                feed_dict = {\n",
        "                    model.Text_a: text1,\n",
        "                    model.Text_b: text2,\n",
        "                    model.Text_neg: text3,\n",
        "                    model.Node_a: node1,\n",
        "                    model.Node_b: node2,\n",
        "                    model.Node_neg: node3,\n",
        "                    model.P_a: P1,\n",
        "                    model.P_b: P2,\n",
        "                    model.P_neg: P3\n",
        "                    #model.labels: labels\n",
        "                }\n",
        "\n",
        "                # Fetch embeddings\n",
        "                #convA, convB, TA, TB = sess.run([model.convA, model.convB, model.N_A, model.N_B])\n",
        "                convA, convB, TA, TB = sess.run([model.convA, model.convB, model.N_A, model.N_B], feed_dict=feed_dict)\n",
        "\n",
        "                # For each node in the batch\n",
        "                for j in range(batch_size):\n",
        "                    em = list(convA[j]) + list(TA[j]) # Create an embedding by concatenating the text (convA) and node (TA) embeddings\n",
        "                    embed[node1[j]].append(em) # A node can appear many times in edges. Thus, each time that node will have a different embedding. Append the different embeddings for a particular node\n",
        "\n",
        "                    em = list(convB[j]) + list(TB[j]) # Create an embedding by concatenating the text (convA) and node (TA) embeddings\n",
        "                    embed[node2[j]].append(em)\n",
        "\n",
        "\n",
        "            for i in range(data.num_nodes):\n",
        "                if embed[i]:\n",
        "                    tmp = np.mean(embed[i], axis=0) #/ len(embed[i]) # If a node has many different embeddings, take their mean.\n",
        "                    file.write((' '.join(map(str, tmp)) + '\\n').encode())\n",
        "                else:\n",
        "                    file.write('\\n'.encode())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZsQv5KyY83v",
        "outputId": "da4038c3-fcfb-4580-b33d-5fd406d184b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auc value: 0.8064516129032258\n"
          ]
        }
      ],
      "source": [
        "node2vec = {}\n",
        "f = open('temp/embed.txt', 'rb')\n",
        "for i, j in enumerate(f):\n",
        "    if j.decode() != '\\n':\n",
        "        node2vec[i] = list(map(float, j.strip().decode().split(' ')))\n",
        "f1 = open(os.path.join('temp/test_graph.txt'), 'rb')\n",
        "edges = [list(map(int, i.strip().decode().split('\\t'))) for i in f1]\n",
        "nodes = list(set([i for j in edges for i in j]))\n",
        "a = 0\n",
        "b = 0\n",
        "result = []\n",
        "for i, j in edges:\n",
        "    if i in node2vec.keys() and j in node2vec.keys():\n",
        "        dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "        random_node = random.sample(nodes, 1)[0]\n",
        "        while random_node == j or random_node not in node2vec.keys():\n",
        "            random_node = random.sample(nodes, 1)[0]\n",
        "        dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "        result.append(np.asarray([dot1,dot2]))\n",
        "        if dot1 > dot2:\n",
        "            a += 1\n",
        "        elif dot1 == dot2:\n",
        "            a += 0.5\n",
        "        b += 1\n",
        "\n",
        "print(\"Auc value:\", float(a) / b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Run (Multiple executions)***"
      ],
      "metadata": {
        "id": "l7BX9wrS3-vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_paths = ['/path/to/graph1.txt']\n",
        "text_paths = ['/path/to/text1.txt', '/path/to/text2.txt']\n",
        "\n",
        "# Log file to save execution details\n",
        "log_file = 'DMTE_execution_logs.txt'\n",
        "\n",
        "# Load data and execute for each combination of graph_path and text_path\n",
        "for graph_path in graph_paths:\n",
        "    for text_path in tqdm(text_paths, desc=\"Processing Text Files\"):\n",
        "      data = dataSet(text_path, graph_path)\n",
        "\n",
        "      # Logging the execution details\n",
        "      with open(log_file, 'a') as log:\n",
        "          log.write(f'Processing graph_path: {graph_path}, text_path: {text_path}\\n')\n",
        "\n",
        "\n",
        "      with tf.Graph().as_default():\n",
        "          sess = tf.compat.v1.Session()\n",
        "          with sess.as_default():\n",
        "              model = Model(data.num_vocab, data.num_nodes, alpha, beta)\n",
        "              opt = tf.compat.v1.train.AdamOptimizer(lr)#tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "              train_op = opt.minimize(model.loss)#opt.minimize(model.loss, var_list=model.trainable_variables)\n",
        "              sess.run(tf.compat.v1.global_variables_initializer())\n",
        "              #total_time = 0\n",
        "\n",
        "              # Training\n",
        "              print('start training.......')\n",
        "              start_time = datetime.now()\n",
        "              for epoch in tqdm(range(num_epoch), desc=\"Epochs\"):\n",
        "                  #start_time = datetime.now()\n",
        "                  loss_epoch = 0\n",
        "                  batches = data.generate_batches()\n",
        "                  h1 = 0\n",
        "                  num_batch = len(batches)\n",
        "                  for i in range(num_batch):\n",
        "                      batch = batches[i]\n",
        "\n",
        "                      node1, node2, node3 = zip(*batch)\n",
        "                      node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                      text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "                      P1, P2, P3 = sub_Mat(data.P, node1), sub_Mat(data.P, node2), sub_Mat(data.P, node3)\n",
        "\n",
        "\n",
        "                      feed_dict = {\n",
        "                          model.Text_a: text1,\n",
        "                          model.Text_b: text2,\n",
        "                          model.Text_neg: text3,\n",
        "                          model.Node_a: node1,\n",
        "                          model.Node_b: node2,\n",
        "                          model.Node_neg: node3,\n",
        "                          model.P_a: P1,\n",
        "                          model.P_b: P2,\n",
        "                          model.P_neg: P3\n",
        "                      }\n",
        "\n",
        "                      # run the graph\n",
        "                      _, loss_batch = sess.run([train_op, model.loss], feed_dict=feed_dict)\n",
        "\n",
        "                      loss_epoch += loss_batch\n",
        "\n",
        "                  #end_time = datetime.now()\n",
        "                  #total_time += (end - start).total_seconds()\n",
        "                  #print('epoch: ', epoch + 1, ' loss: ', loss_epoch)\n",
        "\n",
        "              #print(f'Time: {total_time}')\n",
        "              end_time = datetime.now()\n",
        "              with open(log_file, 'a') as log:\n",
        "                log.write(f'Loss: {loss_epoch}, Time: {(end_time - start_time).total_seconds()}\\n')\n",
        "\n",
        "\n",
        "              # Save embeddings with a unique name\n",
        "              embed_file = f'temp/embed_{os.path.basename(graph_path)}_{os.path.basename(text_path)}.txt'\n",
        "              with open(embed_file, 'wb') as file:\n",
        "                  batches = data.generate_batches(mode='add')\n",
        "                  num_batch = len(batches)\n",
        "                  embed = [[] for _ in range(data.num_nodes)]\n",
        "\n",
        "                  for i in range(num_batch):\n",
        "                      batch = batches[i]\n",
        "                      node1, node2, node3 = zip(*batch)\n",
        "                      node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                      text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "                      P1, P2, P3 = sub_Mat(data.P, node1), sub_Mat(data.P, node2), sub_Mat(data.P, node3)\n",
        "\n",
        "                      feed_dict = {\n",
        "                          model.Text_a: text1,\n",
        "                          model.Text_b: text2,\n",
        "                          model.Text_neg: text3,\n",
        "                          model.Node_a: node1,\n",
        "                          model.Node_b: node2,\n",
        "                          model.Node_neg: node3,\n",
        "                          model.P_a: P1,\n",
        "                          model.P_b: P2,\n",
        "                          model.P_neg: P3\n",
        "                      }\n",
        "\n",
        "                      # Fetch embeddings\n",
        "                      #convA, convB, TA, TB = sess.run([model.convA, model.convB, model.N_A, model.N_B])\n",
        "                      convA, convB, TA, TB = sess.run([model.convA, model.convB, model.N_A, model.N_B], feed_dict=feed_dict)\n",
        "\n",
        "                      # For each node in the batch\n",
        "                      for j in range(batch_size):\n",
        "                          em = list(convA[j]) + list(TA[j]) # Create an embedding by concatenating the text (convA) and node (TA) embeddings\n",
        "                          embed[node1[j]].append(em) # A node can appear many times in edges. Thus, each time that node will have a different embedding. Append the different embeddings for a particular node\n",
        "\n",
        "                          em = list(convB[j]) + list(TB[j]) # Create an embedding by concatenating the text (convA) and node (TA) embeddings\n",
        "                          embed[node2[j]].append(em)\n",
        "\n",
        "\n",
        "                  for i in range(data.num_nodes):\n",
        "                      if embed[i]:\n",
        "                          tmp = np.mean(embed[i], axis=0) #/ len(embed[i]) # If a node has many different embeddings, take their mean.\n",
        "                          file.write((' '.join(map(str, tmp)) + '\\n').encode())\n",
        "                      else:\n",
        "                          file.write('\\n'.encode())\n",
        "              # Log completion\n",
        "              with open(log_file, 'a') as log:\n",
        "                  log.write(f'Embeddings saved to: {embed_file}\\n')\n"
      ],
      "metadata": {
        "id": "qM5K667l4By7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_files = [\"embed1.txt\", \"embed2.txt\", \"embed3.txt\"]\n",
        "test_graph_file = \"temp/test_graph.txt\"\n",
        "\n",
        "# Initialize a log file to store the AUC results\n",
        "with open(\"DMTE_auc_results.log\", \"w\") as auc_file:\n",
        "    auc_file.write(\"Embed File\\tAUC Value\\n\")\n",
        "\n",
        "\n",
        "# Loop through each embed.txt file\n",
        "for embed_file in tqdm(embed_files, desc=\"Processing embed files\"):\n",
        "    node2vec = {}\n",
        "\n",
        "    with open(embed_file, 'rb') as f:\n",
        "      for i, j in enumerate(f):\n",
        "          if j.decode() != '\\n':\n",
        "              node2vec[i] = list(map(float, j.strip().decode().split(' ')))\n",
        "\n",
        "    # Load the edges from the test graph file\n",
        "    with open(test_graph_file, 'rb') as f1:\n",
        "        edges = [list(map(int, i.strip().decode().split('\\t'))) for i in f1]\n",
        "    nodes = list(set([i for j in edges for i in j]))\n",
        "\n",
        "    a = 0\n",
        "    b = 0\n",
        "    result = []\n",
        "    for i, j in edges:\n",
        "        if i in node2vec.keys() and j in node2vec.keys():\n",
        "            dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "            random_node = random.sample(nodes, 1)[0]\n",
        "            while random_node == j or random_node not in node2vec.keys():\n",
        "                random_node = random.sample(nodes, 1)[0]\n",
        "            dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "            result.append(np.asarray([dot1,dot2]))\n",
        "            if dot1 > dot2:\n",
        "                a += 1\n",
        "            elif dot1 == dot2:\n",
        "                a += 0.5\n",
        "            b += 1\n",
        "\n",
        "    auc_value = float(a) / b if b > 0 else 0\n",
        "    print(f\"AUC value for {embed_file}: {auc_value}\")\n",
        "\n",
        "    # Log the result\n",
        "    with open(\"DMTE_auc_results.log\", \"a\") as auc_file:\n",
        "        auc_file.write(f\"{embed_file}\\t{test_graph_file}\\t{auc_value}\\n\")"
      ],
      "metadata": {
        "id": "OT0ngnsv4JVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}